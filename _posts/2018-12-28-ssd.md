---
layout: post
title: SSD
date: 2018-12-28
categories: [æ·±åº¦å­¦ä¹ -è§†è§‰]
tags: æ·±åº¦å­¦ä¹ -è§†è§‰
---
<!--more-->

æ ‡ç­¾ï¼š æ·±åº¦å­¦ä¹ -è§†è§‰

# SSD

---

### [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)

#### SSD:å•å‘å¤šç›’æ£€æµ‹å™¨

---

### Abstract. 
#### æ‘˜è¦
We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines pre- dictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into sys- tems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 Ã— 300 in- put, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 Ã— 512 input, SSD achieves 76.9% mAP, outperforming a compa- rable state-of-the-art Faster R-CNN model. Compared to other single stage meth- ods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .

**æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å•ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¥æ£€æµ‹å›¾åƒä¸­çš„ç›®æ ‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸ºSSDï¼Œå°†è¾¹ç•Œæ¡†çš„è¾“å‡ºç©ºé—´ç¦»æ•£åŒ–ä¸ºä¸åŒé•¿å®½æ¯”çš„ä¸€ç»„é»˜è®¤æ¡†å’Œå¹¶ç¼©æ”¾æ¯ä¸ªç‰¹å¾æ˜ å°„çš„ä½ç½®ã€‚åœ¨é¢„æµ‹æ—¶ï¼Œç½‘ç»œä¼šåœ¨æ¯ä¸ªé»˜è®¤æ¡†ä¸­ä¸ºæ¯ä¸ªç›®æ ‡ç±»åˆ«çš„å‡ºç°ç”Ÿæˆåˆ†æ•°ï¼Œå¹¶å¯¹æ¡†è¿›è¡Œè°ƒæ•´ä»¥æ›´å¥½åœ°åŒ¹é…ç›®æ ‡å½¢çŠ¶ã€‚æ­¤å¤–ï¼Œç½‘ç»œè¿˜ç»“åˆäº†ä¸åŒåˆ†è¾¨ç‡çš„å¤šä¸ªç‰¹å¾æ˜ å°„çš„é¢„æµ‹ï¼Œè‡ªç„¶åœ°å¤„ç†å„ç§å°ºå¯¸çš„ç›®æ ‡ã€‚ç›¸å¯¹äºéœ€è¦ç›®æ ‡å€™é€‰ï¼ˆobject proposalsï¼‰çš„æ–¹æ³•ï¼ŒSSDéå¸¸ç®€å•ï¼Œå› ä¸ºå®ƒå®Œå…¨æ¶ˆé™¤äº†æå‡ºç”Ÿæˆå’Œéšåçš„åƒç´ æˆ–ç‰¹å¾é‡æ–°é‡‡æ ·é˜¶æ®µï¼Œå¹¶å°†æ‰€æœ‰è®¡ç®—å°è£…åˆ°å•ä¸ªç½‘ç»œä¸­ã€‚è¿™ä½¿å¾—SSDæ˜“äºè®­ç»ƒå’Œç›´æ¥é›†æˆåˆ°éœ€è¦æ£€æµ‹ç»„ä»¶çš„ç³»ç»Ÿä¸­ã€‚PASCAL VOCï¼ŒCOCOå’ŒILSVRCæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯å®ï¼ŒSSDå¯¹äºåˆ©ç”¨é¢å¤–çš„ç›®æ ‡æå‡ºæ­¥éª¤çš„æ–¹æ³•å…·æœ‰ç«äº‰æ€§çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”é€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶ä¸ºè®­ç»ƒå’Œæ¨æ–­æä¾›äº†ç»Ÿä¸€çš„æ¡†æ¶ã€‚å¯¹äº300Ã—300çš„è¾“å…¥ï¼ŒSSDåœ¨VOC2007æµ‹è¯•ä¸­ä»¥59FPSçš„é€Ÿåº¦åœ¨Nvidia Titan Xä¸Šè¾¾åˆ°74.3%çš„mAPï¼Œå¯¹äº512Ã—512çš„è¾“å…¥ï¼ŒSSDè¾¾åˆ°äº†76.9%çš„mAPï¼Œä¼˜äºå‚ç…§çš„æœ€å…ˆè¿›çš„Faster R-CNNæ¨¡å‹ã€‚ä¸å…¶ä»–å•é˜¶æ®µæ–¹æ³•ç›¸æ¯”ï¼Œå³ä½¿è¾“å…¥å›¾åƒå°ºå¯¸è¾ƒå°ï¼ŒSSDä¹Ÿå…·æœ‰æ›´é«˜çš„ç²¾åº¦ã€‚ä»£ç è·å–ï¼šhttps://github.com/weiliu89/caffe/tree/ssdã€‚**

Keywords: Real-time Object Detection; Convolutional Neural Network

**å…³é”®è¯ï¼š å®æ—¶ç›®æ ‡æ£€æµ‹ï¼›å·ç§¯ç¥ç»ç½‘ç»œ**

### 1 Introduction
#### 1. å¼•è¨€

Current state-of-the-art object detection systems are variants of the following approach: hypothesize bounding boxes, resample pixels or features for each box, and apply a high- quality classifier. This pipeline has prevailed on detection benchmarks since the Selec- tive Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3]. While accurate, these approaches have been too computationally intensive for em- bedded systems and, even with high-end hardware, too slow for real-time applications.Often detection speed for these approaches is measured in seconds per frame (SPF), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (FPS). There have been many attempts to build faster detectors by attacking each stage of the detection pipeline (see related work in Sec. 4), but so far, significantly increased speed comes only at the cost of significantly decreased detection accuracy.

**ç›®å‰æœ€å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹ç³»ç»Ÿæ˜¯ä»¥ä¸‹æ–¹æ³•çš„å˜ç§ï¼šå‡è®¾è¾¹ç•Œæ¡†ï¼ˆhypothesize bounding boxesï¼‰ï¼Œæ¯ä¸ªæ¡†é‡é‡‡æ ·åƒç´ æˆ–ç‰¹å¾ï¼Œå¹¶åº”ç”¨ä¸€ä¸ªé«˜è´¨é‡çš„åˆ†ç±»å™¨ã€‚è‡ªä»é€‰æ‹©æ€§æœç´¢[1]é€šè¿‡åœ¨PASCAL VOCï¼ŒCOCOå’ŒILSVRCä¸Šæ‰€æœ‰åŸºäºFaster R-CNN[2]çš„æ£€æµ‹éƒ½å–å¾—äº†å½“å‰é¢†å…ˆçš„ç»“æœï¼ˆå°½ç®¡å…·æœ‰æ›´æ·±çš„ç‰¹å¾å¦‚[3]ï¼‰ï¼Œè¿™ç§æµç¨‹åœ¨æ£€æµ‹åŸºå‡†æ•°æ®ä¸Šæµè¡Œå¼€æ¥ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å‡†ç¡®ï¼Œä½†å¯¹äºåµŒå…¥å¼ç³»ç»Ÿè€Œè¨€ï¼Œè¿™äº›æ–¹æ³•çš„è®¡ç®—é‡è¿‡å¤§ï¼Œå³ä½¿æ˜¯é«˜ç«¯ç¡¬ä»¶ï¼Œå¯¹äºå®æ—¶åº”ç”¨è€Œè¨€ä¹Ÿå¤ªæ…¢ã€‚é€šå¸¸ï¼Œè¿™äº›æ–¹æ³•çš„æ£€æµ‹é€Ÿåº¦æ˜¯ä»¥æ¯å¸§ç§’ï¼ˆSPFï¼‰åº¦é‡ï¼Œç”šè‡³æœ€å¿«çš„é«˜ç²¾åº¦æ£€æµ‹å™¨ï¼ŒFaster R-CNNï¼Œä»…ä»¥æ¯ç§’7å¸§ï¼ˆFPSï¼‰çš„é€Ÿåº¦è¿è¡Œã€‚å·²ç»æœ‰å¾ˆå¤šå°è¯•é€šè¿‡å¤„ç†æ£€æµ‹æµç¨‹ä¸­çš„æ¯ä¸ªé˜¶æ®µæ¥æ„å»ºæ›´å¿«çš„æ£€æµ‹å™¨ï¼ˆå‚è§ç¬¬4èŠ‚ä¸­çš„ç›¸å…³å·¥ä½œï¼‰ï¼Œä½†æ˜¯åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ˜¾è‘—æé«˜çš„é€Ÿåº¦ä»…ä»¥æ˜¾è‘—é™ä½çš„æ£€æµ‹ç²¾åº¦ä¸ºä»£ä»·ã€‚**

This paper presents the first deep network based object detector that does not re- sample pixels or features for bounding box hypotheses and and is as accurate as ap- proaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or fea- ture resampling stage. We are not the first to do this (cf [4,5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous at- tempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modificationsâ€”especially using multiple layers for prediction at different scalesâ€”we can achieve high-accuracy using relatively low resolution input, further increasing de- tection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improve- ment in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful.

**æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªåŸºäºæ·±åº¦ç½‘ç»œçš„ç›®æ ‡æ£€æµ‹å™¨ï¼Œå®ƒä¸å¯¹è¾¹ç•Œæ¡†å‡è®¾çš„åƒç´ æˆ–ç‰¹å¾è¿›è¡Œé‡é‡‡æ ·ï¼Œå¹¶ä¸”ä¸å…¶å®ƒæ–¹æ³•æœ‰ä¸€æ ·ç²¾ç¡®åº¦ã€‚è¿™å¯¹é«˜ç²¾åº¦æ£€æµ‹åœ¨é€Ÿåº¦ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼ˆåœ¨VOC2007æµ‹è¯•ä¸­ï¼Œ59FPSå’Œ74.3%çš„mAPï¼Œä¸Faster R-CNN 7FPSå’Œ73.2%çš„mAPæˆ–è€…YOLO 45 FPSå’Œ63.4%çš„mAPç›¸æ¯”ï¼‰ã€‚é€Ÿåº¦çš„æ ¹æœ¬æ”¹è¿›æ¥è‡ªæ¶ˆé™¤è¾¹ç•Œæ¡†æå‡ºå’Œéšåçš„åƒç´ æˆ–ç‰¹å¾é‡é‡‡æ ·é˜¶æ®µã€‚æˆ‘ä»¬å¹¶ä¸æ˜¯ç¬¬ä¸€ä¸ªè¿™æ ·åšçš„äººï¼ˆæŸ¥é˜…[4,5]ï¼‰ï¼Œä½†æ˜¯é€šè¿‡å¢åŠ ä¸€ç³»åˆ—æ”¹è¿›ï¼Œæˆ‘ä»¬è®¾æ³•æ¯”ä»¥å‰çš„å°è¯•æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ”¹è¿›åŒ…æ‹¬ä½¿ç”¨å°å‹å·ç§¯æ»¤æ³¢å™¨æ¥é¢„æµ‹è¾¹ç•Œæ¡†ä½ç½®ä¸­çš„ç›®æ ‡ç±»åˆ«å’Œåç§»é‡ï¼Œä½¿ç”¨ä¸åŒé•¿å®½æ¯”æ£€æµ‹çš„å•ç‹¬é¢„æµ‹å™¨ï¼ˆæ»¤æ³¢å™¨ï¼‰ï¼Œå¹¶å°†è¿™äº›æ»¤æ³¢å™¨åº”ç”¨äºç½‘ç»œåæœŸçš„å¤šä¸ªç‰¹å¾æ˜ å°„ä¸­ï¼Œä»¥æ‰§è¡Œå¤šå°ºåº¦æ£€æµ‹ã€‚é€šè¿‡è¿™äº›ä¿®æ”¹â€”â€”ç‰¹åˆ«æ˜¯ä½¿ç”¨å¤šå±‚è¿›è¡Œä¸åŒå°ºåº¦çš„é¢„æµ‹â€”â€”æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸å¯¹è¾ƒä½çš„åˆ†è¾¨ç‡è¾“å…¥å®ç°é«˜ç²¾åº¦ï¼Œè¿›ä¸€æ­¥æé«˜æ£€æµ‹é€Ÿåº¦ã€‚è™½ç„¶è¿™äº›è´¡çŒ®å¯èƒ½å•ç‹¬çœ‹èµ·æ¥å¾ˆå°ï¼Œä½†æ˜¯æˆ‘ä»¬æ³¨æ„åˆ°ç”±æ­¤äº§ç”Ÿçš„ç³»ç»Ÿå°†PASCAL VOCå®æ—¶æ£€æµ‹çš„å‡†ç¡®åº¦ä»YOLOçš„63.4%çš„mAPæé«˜åˆ°æˆ‘ä»¬çš„SSDçš„74.3%çš„mAPã€‚ç›¸æ¯”äºæœ€è¿‘å¤‡å—ç©ç›®çš„æ®‹å·®ç½‘ç»œæ–¹é¢çš„å·¥ä½œ[3]ï¼Œåœ¨æ£€æµ‹ç²¾åº¦ä¸Šè¿™æ˜¯ç›¸å¯¹æ›´å¤§çš„æé«˜ã€‚è€Œä¸”ï¼Œæ˜¾è‘—æé«˜çš„é«˜è´¨é‡æ£€æµ‹é€Ÿåº¦å¯ä»¥æ‰©å¤§è®¡ç®—æœºè§†è§‰ä½¿ç”¨çš„è®¾ç½®èŒƒå›´ã€‚**

We summarize our contributions as follows:

**æˆ‘ä»¬æ€»ç»“æˆ‘ä»¬çš„è´¡çŒ®å¦‚ä¸‹ï¼š**

 - We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (YOLO), and significantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN).
 - **æˆ‘ä»¬å¼•å…¥äº†SSDï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šä¸ªç±»åˆ«çš„å•æ¬¡æ£€æµ‹å™¨ï¼Œæ¯”å…ˆå‰çš„å…ˆè¿›çš„å•æ¬¡æ£€æµ‹å™¨ï¼ˆYOLOï¼‰æ›´å¿«ï¼Œå¹¶ä¸”å‡†ç¡®å¾—å¤šï¼Œäº‹å®ä¸Šï¼Œä¸æ‰§è¡Œæ˜¾å¼åŒºåŸŸæå‡ºå’Œæ± åŒ–çš„æ›´æ…¢çš„æŠ€æœ¯å…·æœ‰ç›¸åŒçš„ç²¾åº¦ï¼ˆåŒ…æ‹¬Faster R-CNNï¼‰ã€‚**
 - The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. 
 - **SSDçš„æ ¸å¿ƒæ˜¯é¢„æµ‹å›ºå®šçš„ä¸€ç³»åˆ—é»˜è®¤è¾¹ç•Œæ¡†çš„ç±»åˆ«åˆ†æ•°å’Œè¾¹ç•Œæ¡†åç§»ï¼Œä½¿ç”¨æ›´å°çš„å·ç§¯æ»¤æ³¢å™¨åº”ç”¨åˆ°ç‰¹å¾æ˜ å°„ä¸Šã€‚**
 - To achieve high detection accuracy we produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.
 - **ä¸ºäº†å®ç°é«˜æ£€æµ‹ç²¾åº¦ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒå°ºåº¦çš„ç‰¹å¾æ˜ å°„ç”Ÿæˆä¸åŒå°ºåº¦çš„é¢„æµ‹ï¼Œå¹¶é€šè¿‡çºµæ¨ªæ¯”æ˜ç¡®åˆ†å¼€é¢„æµ‹ã€‚**
 - These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.
 - **è¿™äº›è®¾è®¡åŠŸèƒ½ä½¿å¾—å³ä½¿åœ¨ä½åˆ†è¾¨ç‡è¾“å…¥å›¾åƒä¸Šä¹Ÿèƒ½å®ç°ç®€å•çš„ç«¯åˆ°ç«¯è®­ç»ƒå’Œé«˜ç²¾åº¦ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜é€Ÿåº¦ä¸ç²¾åº¦ä¹‹é—´çš„æƒè¡¡ã€‚**
 - Experiments include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches.
 - **å®éªŒåŒ…æ‹¬åœ¨PASCAL VOCï¼ŒCOCOå’ŒILSVRCä¸Šè¯„ä¼°å…·æœ‰ä¸åŒè¾“å…¥å¤§å°çš„æ¨¡å‹çš„æ—¶é—´å’Œç²¾åº¦åˆ†æï¼Œå¹¶ä¸æœ€è¿‘çš„ä¸€ç³»åˆ—æœ€æ–°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚**
 
### 2 The Single Shot Detector (SSD)
#### 2 å•æ¬¡æ£€æµ‹å™¨

This section describes our proposed SSD framework for detection (Sec. 2.1) and the associated training methodology (Sec. 2.2). Afterwards, Sec. 3 presents dataset-specific model details and experimental results.

**æœ¬èŠ‚æè¿°æˆ‘ä»¬æå‡ºçš„SSDæ£€æµ‹æ¡†æ¶ï¼ˆ2.1èŠ‚ï¼‰å’Œç›¸å…³çš„è®­ç»ƒæ–¹æ³•ï¼ˆ2.2èŠ‚ï¼‰ã€‚ä¹‹åï¼Œ2.3èŠ‚ä»‹ç»äº†æ•°æ®é›†ç‰¹æœ‰çš„æ¨¡å‹ç»†èŠ‚å’Œå®éªŒç»“æœã€‚**

---

#### 2.1 Model
#### 2.1 æ¨¡å‹

The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network2. We then add auxiliary structure to the network to produce detections with the following key features:

**SSDæ–¹æ³•åŸºäºå‰é¦ˆå·ç§¯ç½‘ç»œï¼Œè¯¥ç½‘ç»œäº§ç”Ÿå›ºå®šå¤§å°çš„è¾¹ç•Œæ¡†é›†åˆï¼Œå¹¶å¯¹è¿™äº›è¾¹ç•Œæ¡†ä¸­å­˜åœ¨çš„ç›®æ ‡ç±»åˆ«å®ä¾‹è¿›è¡Œè¯„åˆ†ï¼Œç„¶åè¿›è¡Œéæå¤§å€¼æŠ‘åˆ¶æ­¥éª¤æ¥äº§ç”Ÿæœ€ç»ˆçš„æ£€æµ‹ç»“æœã€‚æ—©æœŸçš„ç½‘ç»œå±‚åŸºäºç”¨äºé«˜è´¨é‡å›¾åƒåˆ†ç±»çš„æ ‡å‡†æ¶æ„ï¼ˆåœ¨ä»»ä½•åˆ†ç±»å±‚ä¹‹å‰è¢«æˆªæ–­ï¼‰ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºåŸºç¡€ç½‘ç»œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¾…åŠ©ç»“æ„æ·»åŠ åˆ°ç½‘ç»œä¸­ä»¥äº§ç”Ÿå…·æœ‰ä»¥ä¸‹å…³é”®ç‰¹å¾çš„æ£€æµ‹ï¼š**

```Multi-scale feature maps for detection``` We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).

**```ç”¨äºæ£€æµ‹çš„å¤šå°ºåº¦ç‰¹å¾æ˜ å°„```ã€‚æˆ‘ä»¬å°†å·ç§¯ç‰¹å¾å±‚æ·»åŠ åˆ°æˆªå–çš„åŸºç¡€ç½‘ç»œçš„æœ«ç«¯ã€‚è¿™äº›å±‚åœ¨å°ºå¯¸ä¸Šé€æ¸å‡å°ï¼Œå¹¶å…è®¸åœ¨å¤šä¸ªå°ºåº¦ä¸Šå¯¹æ£€æµ‹ç»“æœè¿›è¡Œé¢„æµ‹ã€‚ç”¨äºé¢„æµ‹æ£€æµ‹çš„å·ç§¯æ¨¡å‹å¯¹äºæ¯ä¸ªç‰¹å¾å±‚éƒ½æ˜¯ä¸åŒçš„ï¼ˆæŸ¥é˜…Overfeat[4]å’ŒYOLO[5]åœ¨å•å°ºåº¦ç‰¹å¾æ˜ å°„ä¸Šçš„æ“ä½œï¼‰ã€‚**

```Convolutional predictors for detection``` Each added feature layer (or optionally an ex- isting feature layer from the base network) can produce a fixed set of detection predic- tions using a set of convolutional filters. These are indicated on top of the SSD network architecture in Fig. 2. For a feature layer of size m Ã— n with p channels, the basic el- ement for predicting parameters of a potential detection is a 3 Ã— 3 Ã— p small kernel that produces either a score for a category, or a shape offset relative to the default box coordinates. At each of the m Ã— n locations where the kernel is applied, it produces an output value. The bounding box offset output values are measured relative to a default box position relative to each feature map location (cf the architecture of YOLO[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).

**```ç”¨äºæ£€æµ‹çš„å·ç§¯é¢„æµ‹å™¨```ã€‚æ¯ä¸ªæ·»åŠ çš„ç‰¹å¾å±‚ï¼ˆæˆ–è€…ä»»é€‰çš„æ¥è‡ªåŸºç¡€ç½‘ç»œçš„ç°æœ‰ç‰¹å¾å±‚ï¼‰å¯ä»¥ä½¿ç”¨ä¸€ç»„å·ç§¯æ»¤æ³¢å™¨äº§ç”Ÿå›ºå®šçš„æ£€æµ‹é¢„æµ‹é›†åˆã€‚è¿™äº›åœ¨å›¾2ä¸­çš„SSDç½‘ç»œæ¶æ„çš„ä¸Šéƒ¨æŒ‡å‡ºã€‚å¯¹äºå…·æœ‰pé€šé“çš„å¤§å°ä¸ºmÃ—nçš„ç‰¹å¾å±‚ï¼Œæ½œåœ¨æ£€æµ‹çš„é¢„æµ‹å‚æ•°çš„åŸºæœ¬å…ƒç´ æ˜¯3Ã—3Ã—pçš„å°æ ¸å¾—åˆ°æŸä¸ªç±»åˆ«çš„åˆ†æ•°ï¼Œæˆ–è€…ç›¸å¯¹äºé»˜è®¤æ¡†åæ ‡çš„å½¢çŠ¶åç§»ã€‚åœ¨åº”ç”¨å·ç§¯æ ¸çš„mÃ—nçš„æ¯ä¸ªä½ç½®ï¼Œå®ƒä¼šäº§ç”Ÿä¸€ä¸ªè¾“å‡ºå€¼ã€‚è¾¹ç•Œæ¡†åç§»è¾“å‡ºå€¼æ˜¯ç›¸å¯¹æ¯ä¸ªç‰¹å¾æ˜ å°„ä½ç½®çš„ç›¸å¯¹é»˜è®¤æ¡†ä½ç½®æ¥åº¦é‡çš„ï¼ˆæŸ¥é˜…YOLO[5]çš„æ¶æ„ï¼Œè¯¥æ­¥éª¤ä½¿ç”¨ä¸­é—´å…¨è¿æ¥å±‚è€Œä¸æ˜¯å·ç§¯æ»¤æ³¢å™¨ï¼‰ã€‚**

![1](/images/posts/ssd/1.png)

Fig. 2: A comparison between two single shot detection models: SSD and YOLO [5]. Our SSD model adds several feature layers to the end of a base network, which predict the offsets to default boxes of different scales and aspect ratios and their associated confidences. SSD with a 300 Ã— 300 input size significantly outperforms its 448 Ã— 448 YOLO counterpart in accuracy on VOC2007 test while also improving the speed.

**å›¾2ï¼šä¸¤ä¸ªå•æ¬¡æ£€æµ‹æ¨¡å‹çš„æ¯”è¾ƒï¼šSSDå’ŒYOLO[5]ã€‚æˆ‘ä»¬çš„SSDæ¨¡å‹åœ¨åŸºç¡€ç½‘ç»œçš„æœ«ç«¯æ·»åŠ äº†å‡ ä¸ªç‰¹å¾å±‚ï¼Œå®ƒé¢„æµ‹äº†ä¸åŒå°ºåº¦å’Œé•¿å®½æ¯”çš„é»˜è®¤è¾¹ç•Œæ¡†çš„åç§»é‡åŠå…¶ç›¸å…³çš„ç½®ä¿¡åº¦ã€‚300Ã—300è¾“å…¥å°ºå¯¸çš„SSDåœ¨VOC2007 testä¸Šçš„å‡†ç¡®åº¦ä¸Šæ˜æ˜¾ä¼˜äº448Ã—448çš„YOLOçš„å‡†ç¡®åº¦ï¼ŒåŒæ—¶ä¹Ÿæé«˜äº†é€Ÿåº¦ã€‚**

```Default boxes and aspect ratios``` We associate a set of default bounding boxes with each feature map cell, for multiple feature maps at the top of the network. The default boxes tile the feature map in a convolutional manner, so that the position of each box relative to its corresponding cell is fixed. At each feature map cell, we predict the offsets relative to the default box shapes in the cell, as well as the per-class scores that indicate the presence of a class instance in each of those boxes. Specifically, for each box out of k at a given location, we compute c class scores and the 4 offsets relative to the original default box shape. This results in a total of (c + 4)k filters that are applied around each location in the feature map, yielding (c + 4)kmn outputs for a m Ã— n feature map. For an illustration of default boxes, please refer to Fig. 1. Our default boxes are similar to the anchor boxes used in Faster R-CNN [2], however we apply them to several feature maps of different resolutions. Allowing different default box shapes in several feature maps let us efficiently discretize the space of possible output box shapes.

**```é»˜è®¤è¾¹ç•Œæ¡†å’Œé•¿å®½æ¯”```ã€‚å¯¹äºç½‘ç»œé¡¶éƒ¨çš„å¤šä¸ªç‰¹å¾æ˜ å°„ï¼Œæˆ‘ä»¬å°†ä¸€ç»„é»˜è®¤è¾¹ç•Œæ¡†ä¸æ¯ä¸ªç‰¹å¾æ˜ å°„å•å…ƒç›¸å…³è”ã€‚é»˜è®¤è¾¹ç•Œæ¡†ä»¥å·ç§¯çš„æ–¹å¼å¹³é“ºç‰¹å¾æ˜ å°„ï¼Œä»¥ä¾¿æ¯ä¸ªè¾¹ç•Œæ¡†ç›¸å¯¹äºå…¶å¯¹åº”å•å…ƒçš„ä½ç½®æ˜¯å›ºå®šçš„ã€‚åœ¨æ¯ä¸ªç‰¹å¾æ˜ å°„å•å…ƒä¸­ï¼Œæˆ‘ä»¬é¢„æµ‹å•å…ƒä¸­ç›¸å¯¹äºé»˜è®¤è¾¹ç•Œæ¡†å½¢çŠ¶çš„åç§»é‡ï¼Œä»¥åŠæŒ‡å‡ºæ¯ä¸ªè¾¹ç•Œæ¡†ä¸­å­˜åœ¨çš„æ¯ä¸ªç±»åˆ«å®ä¾‹çš„ç±»åˆ«åˆ†æ•°ã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºç»™å®šä½ç½®å¤„çš„$k$ä¸ªè¾¹ç•Œæ¡†ä¸­çš„æ¯ä¸€ä¸ªï¼Œæˆ‘ä»¬è®¡ç®—cä¸ªç±»åˆ«åˆ†æ•°å’Œç›¸å¯¹äºåŸå§‹é»˜è®¤è¾¹ç•Œæ¡†å½¢çŠ¶çš„$4$ä¸ªåç§»é‡ã€‚è¿™å¯¼è‡´åœ¨ç‰¹å¾æ˜ å°„ä¸­çš„æ¯ä¸ªä½ç½®å‘¨å›´åº”ç”¨æ€»å…±$(c+4)k$ä¸ªæ»¤æ³¢å™¨ï¼Œå¯¹äºmÃ—nçš„ç‰¹å¾æ˜ å°„å–å¾—$(c+4)kmn$ä¸ªè¾“å‡ºã€‚æœ‰å…³é»˜è®¤è¾¹ç•Œæ¡†çš„è¯´æ˜ï¼Œè¯·å‚è§å›¾1ã€‚æˆ‘ä»¬çš„é»˜è®¤è¾¹ç•Œæ¡†ä¸Faster R-CNN[2]ä¸­ä½¿ç”¨çš„é”šè¾¹ç•Œæ¡†ç›¸ä¼¼ï¼Œä½†æ˜¯æˆ‘ä»¬å°†å®ƒä»¬åº”ç”¨åˆ°ä¸åŒåˆ†è¾¨ç‡çš„å‡ ä¸ªç‰¹å¾æ˜ å°„ä¸Šã€‚åœ¨å‡ ä¸ªç‰¹å¾æ˜ å°„ä¸­å…è®¸ä¸åŒçš„é»˜è®¤è¾¹ç•Œæ¡†å½¢çŠ¶è®©æˆ‘ä»¬æœ‰æ•ˆåœ°ç¦»æ•£å¯èƒ½çš„è¾“å‡ºæ¡†å½¢çŠ¶çš„ç©ºé—´ã€‚**

![2](/images/posts/ssd/2.png)

```Fig. 1: SSD framework.``` (a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 Ã— 8 and 4 Ã— 4 in (b) and (c)). For each default box, we predict both the shape offsets and the confidences for all object categories ((c1 , c2 , Â· Â· Â· , cp )). At training time, we first match these default boxes to the ground truth boxes. For example, we have matched two default boxes with the cat and one with the dog, which are treated as positives and the rest as negatives. The model loss is a weighted sum between localization loss (e.g. Smooth L1 [6]) and confidence loss (e.g. Softmax).

**```å›¾1ï¼šSSDæ¡†æ¶```ã€‚ï¼ˆaï¼‰åœ¨è®­ç»ƒæœŸé—´ï¼ŒSSDä»…éœ€è¦æ¯ä¸ªç›®æ ‡çš„è¾“å…¥å›¾åƒå’ŒçœŸå®è¾¹ç•Œæ¡†ã€‚ä»¥å·ç§¯æ–¹å¼ï¼Œæˆ‘ä»¬è¯„ä¼°å…·æœ‰ä¸åŒå°ºåº¦ï¼ˆä¾‹å¦‚ï¼ˆbï¼‰å’Œï¼ˆcï¼‰ä¸­çš„8Ã—8å’Œ4Ã—4ï¼‰çš„å‡ ä¸ªç‰¹å¾æ˜ å°„ä¸­æ¯ä¸ªä½ç½®å¤„ä¸åŒé•¿å®½æ¯”çš„é»˜è®¤æ¡†çš„å°é›†åˆï¼ˆä¾‹å¦‚4ä¸ªï¼‰ã€‚å¯¹äºæ¯ä¸ªé»˜è®¤è¾¹ç•Œæ¡†ï¼Œæˆ‘ä»¬é¢„æµ‹æ‰€æœ‰ç›®æ ‡ç±»åˆ«$ï¼ˆ(c1,c2,â€¦,cp)ï¼‰$çš„å½¢çŠ¶åç§»é‡å’Œç½®ä¿¡åº¦ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆå°†è¿™äº›é»˜è®¤è¾¹ç•Œæ¡†ä¸å®é™…çš„è¾¹ç•Œæ¡†è¿›è¡ŒåŒ¹é…ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å·²ç»ä¸çŒ«åŒ¹é…ä¸¤ä¸ªé»˜è®¤è¾¹ç•Œæ¡†ï¼Œä¸ç‹—åŒ¹é…äº†ä¸€ä¸ªï¼Œè¿™è¢«è§†ä¸ºç§¯æçš„ï¼Œå…¶ä½™çš„æ˜¯æ¶ˆæçš„ã€‚æ¨¡å‹æŸå¤±æ˜¯```å®šä½æŸå¤±```ï¼ˆä¾‹å¦‚ï¼ŒSmooth L1[6]ï¼‰å’Œ```ç½®ä¿¡åº¦æŸå¤±```ï¼ˆä¾‹å¦‚Softmaxï¼‰ä¹‹é—´çš„åŠ æƒå’Œã€‚**

---

#### 2.2 Training
#### 2.2 è®­ç»ƒ

The key difference between training SSD and training a typical detector that uses region proposals, is that ground truth information needs to be assigned to specific outputs in the fixed set of detector outputs. Some version of this is also required for training in YOLO[5] and for the region proposal stage of Faster R-CNN[2] and MultiBox[7]. Once this assignment is determined, the loss function and back propagation are applied end- to-end. Training also involves choosing the set of default boxes and scales for detection as well as the hard negative mining and data augmentation strategies.

**è®­ç»ƒSSDå’Œè®­ç»ƒä½¿ç”¨åŒºåŸŸæå‡º(region proposal)çš„å…¸å‹æ£€æµ‹å™¨ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºï¼Œéœ€è¦å°†çœŸå®ä¿¡æ¯åˆ†é…ç»™å›ºå®šçš„æ£€æµ‹å™¨è¾“å‡ºé›†åˆä¸­çš„ç‰¹å®šè¾“å‡ºã€‚åœ¨YOLO[5]çš„è®­ç»ƒä¸­ã€Faster R-CNN[2]å’ŒMultiBox[7]çš„åŒºåŸŸæå‡ºé˜¶æ®µï¼Œä¸€äº›ç‰ˆæœ¬ä¹Ÿéœ€è¦è¿™æ ·çš„æ“ä½œã€‚ä¸€æ—¦ç¡®å®šäº†è¿™ä¸ªåˆ†é…ï¼ŒæŸå¤±å‡½æ•°å’Œåå‘ä¼ æ’­å°±å¯ä»¥åº”ç”¨ç«¯åˆ°ç«¯äº†ã€‚è®­ç»ƒä¹Ÿæ¶‰åŠé€‰æ‹©é»˜è®¤è¾¹ç•Œæ¡†é›†åˆå’Œç¼©æ”¾è¿›è¡Œæ£€æµ‹ï¼Œä»¥åŠéš¾ä¾‹æŒ–æ˜å’Œæ•°æ®å¢å¼ºç­–ç•¥ã€‚**

Matching strategy``` During training we need to determine which default boxes corre- spond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.

**```åŒ¹é…ç­–ç•¥```ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šå“ªäº›é»˜è®¤è¾¹ç•Œæ¡†å¯¹åº”å®é™…è¾¹ç•Œæ¡†çš„æ£€æµ‹ï¼Œå¹¶ç›¸åº”åœ°è®­ç»ƒç½‘ç»œã€‚å¯¹äºæ¯ä¸ªå®é™…è¾¹ç•Œæ¡†ï¼Œæˆ‘ä»¬ä»é»˜è®¤è¾¹ç•Œæ¡†ä¸­é€‰æ‹©ï¼Œè¿™äº›æ¡†ä¼šåœ¨ä½ç½®ï¼Œé•¿å®½æ¯”å’Œå°ºåº¦ä¸Šå˜åŒ–ã€‚æˆ‘ä»¬é¦–å…ˆå°†æ¯ä¸ªå®é™…è¾¹ç•Œæ¡†ä¸å…·æœ‰æœ€å¥½çš„Jaccardé‡å ï¼ˆå¦‚MultiBox[7]ï¼‰çš„è¾¹ç•Œæ¡†ç›¸åŒ¹é…ã€‚ä¸MultiBoxä¸åŒçš„æ˜¯ï¼Œ```æˆ‘ä»¬å°†é»˜è®¤è¾¹ç•Œæ¡†åŒ¹é…åˆ°Jaccardé‡å é«˜äºé˜ˆå€¼ï¼ˆ0.5ï¼‰çš„ä»»ä½•å®é™…è¾¹ç•Œæ¡†```ã€‚è¿™ç®€åŒ–äº†å­¦ä¹ é—®é¢˜ï¼Œå…è®¸ç½‘ç»œä¸ºå¤šä¸ªé‡å çš„é»˜è®¤è¾¹ç•Œæ¡†é¢„æµ‹é«˜åˆ†ï¼Œè€Œä¸æ˜¯è¦æ±‚å®ƒåªæŒ‘é€‰å…·æœ‰æœ€å¤§é‡å çš„ä¸€ä¸ªè¾¹ç•Œæ¡†ã€‚**


Training objective The SSD training objective is derived from the MultiBox objective[7,8]but is extended to handle multiple object categories. Let $x_{ij}^p = \lbrace 1,0 \rbrace$ be an indicator for matching the $i$-th default box to the $j$-th ground truth box of category $p$. In the matching strategy above, we can have  $\sum_i x_{ij}^p \geq 1$. The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):
**è®­ç»ƒç›®æ ‡å‡½æ•°ã€‚SSDè®­ç»ƒç›®æ ‡å‡½æ•°æ¥è‡ªäºMultiBoxç›®æ ‡[7,8]ï¼Œä½†æ‰©å±•åˆ°å¤„ç†å¤šä¸ªç›®æ ‡ç±»åˆ«ã€‚è®¾$x_{ij}^p = \lbrace 1,0 \rbrace$æ˜¯ç¬¬$i$ä¸ªé»˜è®¤è¾¹ç•Œæ¡†åŒ¹é…åˆ°ç±»åˆ«$p$çš„ç¬¬$j$ä¸ªå®é™…è¾¹ç•Œæ¡†çš„æŒ‡ç¤ºå™¨ã€‚åœ¨ä¸Šé¢çš„åŒ¹é…ç­–ç•¥ä¸­ï¼Œæˆ‘ä»¬æœ‰$\sum_i x_{ij}^p \geq 1$ã€‚æ€»ä½“ç›®æ ‡æŸå¤±å‡½æ•°æ˜¯å®šä½æŸå¤±ï¼ˆlocï¼‰å’Œç½®ä¿¡åº¦æŸå¤±ï¼ˆconfï¼‰çš„åŠ æƒå’Œï¼š**

$$L(x, c, l, g) = \frac{1}{N}(L_{conf}(x, c) + \alpha L_{loc}(x, l, g)) \tag{1}$$

where N is the number of matched default boxes. If N = 0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters. Similar to Faster R-CNN [2], we regress to offsets for the center (cx, cy) of the default bounding box (d) and for its width (w) and height (h).
**å…¶ä¸­$N$æ˜¯åŒ¹é…çš„é»˜è®¤è¾¹ç•Œæ¡†çš„æ•°é‡ã€‚å¦‚æœ$N=0$ï¼Œåˆ™å°†æŸå¤±è®¾ä¸º0ã€‚å®šä½æŸå¤±æ˜¯é¢„æµ‹æ¡†$(l)$ä¸çœŸå®æ¡†$(g)$å‚æ•°ä¹‹é—´çš„Smooth L1æŸå¤±[6]ã€‚ç±»ä¼¼äºFaster R-CNN[2]ï¼Œæˆ‘ä»¬å›å½’é»˜è®¤è¾¹ç•Œæ¡†$(d)$çš„ä¸­å¿ƒåç§»é‡$(cx,cy)$å’Œå…¶å®½åº¦$(w)$ã€é«˜åº¦$(h)$çš„åç§»é‡ã€‚**

$$L_{loc}(x,l,g) = \sum_{i \in Pos}^N \sum_{m \in \lbrace cx, cy, w, h \rbrace} x_{ij}^k \mathtt{smooth}_{L1}(l_{i}^m - \hat{g}_j^m) \\ \hat{g}_j^{cx} = (g_j^{cx} - d_i^{cx}) / d_i^w \quad \quad \hat{g}_j^{cy} = (g_j^{cy} - d_i^{cy}) / d_i^h \\ \hat{g}_j^{w} = \log\Big(\frac{g_j^{w}}{d_i^w}\Big) \quad \quad \hat{g}_j^{h} = \log\Big(\frac{g_j^{h}}{d_i^h}\Big)  \tag{2}$$

The confidence loss is the softmax loss over multiple classes confidences (c).
**ç½®ä¿¡åº¦æŸå¤±æ˜¯åœ¨å¤šç±»åˆ«ç½®ä¿¡åº¦(c)ä¸Šçš„softmaxæŸå¤±ã€‚**

$$L_{conf}(x, c) = - \sum_{i\in Pos}^N x_{ij}^p log(\hat{c}_i^p) - \sum_{i\in Neg} log(\hat{c}_i^0)\quad \mathtt{where}\quad\hat{c}_i^p = \frac{\exp(c_i^p)}{\sum_p \exp(c_i^p)} \tag{3}$$

and the weight term $Î±$ is set to 1 by cross validation.
**é€šè¿‡äº¤å‰éªŒè¯æƒé‡é¡¹$Î±$è®¾ä¸º1ã€‚**

```Choosing scales and aspect ratios for default boxes``` To handle different object scales, some methods [4,9] suggest processing the image at different sizes and combining the results afterwards. However, by utilizing feature maps from several different layers in a single network for prediction we can mimic the same effect, while also sharing parame- ters across all object scales. Previous works [10,11] have shown that using feature maps from the lower layers can improve semantic segmentation quality because the lower layers capture more fine details of the input objects. Similarly, [12] showed that adding global context pooled from a feature map can help smooth the segmentation results.Motivated by these methods, we use both the lower and upper feature maps for detec- tion. Figure 1 shows two exemplar feature maps (8 Ã— 8 and 4 Ã— 4) which are used in the framework. In practice, we can use many more with small computational overhead.

**ä¸ºé»˜è®¤è¾¹ç•Œæ¡†é€‰æ‹©å°ºåº¦å’Œé•¿å®½æ¯”ã€‚ä¸ºäº†å¤„ç†ä¸åŒçš„ç›®æ ‡å°ºåº¦ï¼Œä¸€äº›æ–¹æ³•[4,9]å»ºè®®å¤„ç†ä¸åŒå°ºå¯¸çš„å›¾åƒï¼Œç„¶åå°†ç»“æœåˆå¹¶ã€‚ç„¶è€Œï¼Œé€šè¿‡åˆ©ç”¨å•ä¸ªç½‘ç»œä¸­å‡ ä¸ªä¸åŒå±‚çš„ç‰¹å¾æ˜ å°„è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿç›¸åŒçš„æ•ˆæœï¼ŒåŒæ—¶è¿˜å¯ä»¥è·¨æ‰€æœ‰ç›®æ ‡å°ºåº¦å…±äº«å‚æ•°ã€‚ä»¥å‰çš„å·¥ä½œ[10,11]å·²ç»è¡¨æ˜ï¼Œä½¿ç”¨ä½å±‚çš„ç‰¹å¾æ˜ å°„å¯ä»¥æé«˜è¯­ä¹‰åˆ†å‰²çš„è´¨é‡ï¼Œå› ä¸ºä½å±‚ä¼šæ•è·è¾“å…¥ç›®æ ‡çš„æ›´å¤šç»†èŠ‚ã€‚åŒæ ·ï¼Œ[12]è¡¨æ˜ï¼Œä»ç‰¹å¾æ˜ å°„ä¸Šæ·»åŠ å…¨å±€ä¸Šä¸‹æ–‡æ± åŒ–å¯ä»¥æœ‰åŠ©äºå¹³æ»‘åˆ†å‰²ç»“æœã€‚å—è¿™äº›æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾ƒä½å’Œè¾ƒé«˜çš„ç‰¹å¾æ˜ å°„è¿›è¡Œæ£€æµ‹ã€‚å›¾1æ˜¾ç¤ºäº†æ¡†æ¶ä¸­ä½¿ç”¨çš„ä¸¤ä¸ªç¤ºä¾‹æ€§ç‰¹å¾æ˜ å°„ï¼ˆ8Ã—8å’Œ4Ã—4ï¼‰ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤šçš„å…·æœ‰å¾ˆå°‘è®¡ç®—å¼€æ”¯çš„ç‰¹å¾æ˜ å°„ã€‚**

Feature maps from different levels within a network are known to have different (empirical) receptive field sizes [13]. Fortunately, within the SSD framework, the de- fault boxes do not necessary need to correspond to the actual receptive fields of each layer. We design the tiling of default boxes so that specific feature maps learn to be responsive to particular scales of the objects. Suppose we want to use m feature maps for prediction. The scale of the default boxes for each feature map is computed as:

**å·²çŸ¥ç½‘ç»œä¸­ä¸åŒå±‚çš„ç‰¹å¾æ˜ å°„å…·æœ‰ä¸åŒçš„ï¼ˆç»éªŒçš„ï¼‰æ„Ÿå—é‡å¤§å°[13]ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨SSDæ¡†æ¶å†…ï¼Œé»˜è®¤è¾¹ç•Œæ¡†ä¸éœ€è¦å¯¹åº”äºæ¯å±‚çš„å®é™…æ„Ÿå—é‡ã€‚æˆ‘ä»¬è®¾è®¡å¹³é“ºé»˜è®¤è¾¹ç•Œæ¡†ï¼Œä»¥ä¾¿ç‰¹å®šçš„ç‰¹å¾æ˜ å°„å­¦ä¹ å“åº”ç›®æ ‡çš„ç‰¹å®šå°ºåº¦ã€‚å‡è®¾æˆ‘ä»¬è¦ä½¿ç”¨$m$ä¸ªç‰¹å¾æ˜ å°„è¿›è¡Œé¢„æµ‹ã€‚æ¯ä¸ªç‰¹å¾æ˜ å°„é»˜è®¤è¾¹ç•Œæ¡†çš„å°ºåº¦è®¡ç®—å¦‚ä¸‹ï¼š**

$$s_k = s_\text{min} + \frac{s_\text{max} - s_\text{min}}{m - 1} (k - 1),\quad k\in [1, m]$$

where $s_\text{min}$ is 0.2 and $s_\text{max}$ is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced. We impose different aspect ratios for the default boxes, and denote them as $a_r \in {1, 2, 3, \frac{1}{2}, \frac{1}{3}}$. We can compute the width ($w_k^a = s_k\sqrt{a_r}$) and height $h_k^a = s_k / \sqrt{a_r}$ for each default box. For the aspect ratio of 1, we also add a default box whose scale is
$sâ€™_k = \sqrt{s_k s_{k+1}}$, resulting in 6 default boxes per feature map location. We set the center of each default box to $(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$, where $|f_k|$ is the size of the k-th square feature map, $i, j\in [0, |f_k|)$. In practice, one can also design a distribution of default boxes to best fit a specific dataset. How to design the optimal tiling is an open question as well.

**å…¶ä¸­$s_\text{min}$ä¸º0.2ï¼Œ$s_\text{max}$ä¸º0.9ï¼Œæ„å‘³ç€æœ€ä½å±‚å…·æœ‰0.2çš„å°ºåº¦ï¼Œæœ€é«˜å±‚å…·æœ‰0.9çš„å°ºåº¦ï¼Œå¹¶ä¸”åœ¨å®ƒä»¬ä¹‹é—´çš„æ‰€æœ‰å±‚æ˜¯è§„åˆ™é—´éš”çš„ã€‚æˆ‘ä»¬ä¸ºé»˜è®¤è¾¹ç•Œæ¡†æ·»åŠ ä¸åŒçš„é•¿å®½æ¯”ï¼Œå¹¶å°†å®ƒä»¬è¡¨ç¤ºä¸º$a_r \in {1, 2, 3, \frac{1}{2}, \frac{1}{3}}$ã€‚æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªè¾¹ç•Œæ¡†çš„å®½åº¦($w_k^a = s_k\sqrt{a_r}$)å’Œé«˜åº¦($h_k^a = s_k / \sqrt{a_r}$)ã€‚å¯¹äºé•¿å®½æ¯”ä¸º1ï¼Œæˆ‘ä»¬è¿˜æ·»åŠ äº†ä¸€ä¸ªé»˜è®¤è¾¹ç•Œæ¡†ï¼Œå…¶å°ºåº¦ä¸º$sâ€™_k = \sqrt{s_k s_{k+1}}$ï¼Œåœ¨æ¯ä¸ªç‰¹å¾æ˜ å°„ä½ç½®å¾—åˆ°6ä¸ªé»˜è®¤è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬å°†æ¯ä¸ªé»˜è®¤è¾¹ç•Œæ¡†çš„ä¸­å¿ƒè®¾ç½®ä¸º($(\frac{i+0.5}{|f_k|}, \frac{j+0.5}{|f_k|})$)ï¼Œå…¶ä¸­$|f_k|$æ˜¯ç¬¬kä¸ªå¹³æ–¹ç‰¹å¾æ˜ å°„çš„å¤§å°ï¼Œ $i, j\in [0, |f_k|)$ ã€‚åœ¨å®è·µä¸­ï¼Œä¹Ÿå¯ä»¥è®¾è®¡é»˜è®¤è¾¹ç•Œæ¡†çš„åˆ†å¸ƒä»¥æœ€é€‚åˆç‰¹å®šçš„æ•°æ®é›†ã€‚å¦‚ä½•è®¾è®¡æœ€ä½³å¹³é“ºä¹Ÿæ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚**

By combining predictions for all default boxes with different scales and aspect ratios from all locations of many feature maps, we have a diverse set of predictions, covering various input object sizes and shapes. For example, in Fig. 1, the dog is matched to a default box in the 4 Ã— 4 feature map, but not to any default boxes in the 8 Ã— 8 feature map. This is because those boxes have different scales and do not match the dog box, and therefore are considered as negatives during training.

**é€šè¿‡å°†æ‰€æœ‰é»˜è®¤è¾¹ç•Œæ¡†çš„é¢„æµ‹ä¸è®¸å¤šç‰¹å¾æ˜ å°„æ‰€æœ‰ä½ç½®çš„ä¸åŒå°ºåº¦å’Œé«˜å®½æ¯”ç›¸ç»“åˆï¼Œæˆ‘ä»¬æœ‰ä¸åŒçš„é¢„æµ‹é›†åˆï¼Œæ¶µç›–å„ç§è¾“å…¥ç›®æ ‡å¤§å°å’Œå½¢çŠ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ä¸­ï¼Œç‹—è¢«åŒ¹é…åˆ°4Ã—4ç‰¹å¾æ˜ å°„ä¸­çš„é»˜è®¤è¾¹ç•Œæ¡†ï¼Œè€Œä¸æ˜¯8Ã—8ç‰¹å¾æ˜ å°„ä¸­çš„ä»»ä½•é»˜è®¤æ¡†ã€‚è¿™æ˜¯å› ä¸ºé‚£äº›è¾¹ç•Œæ¡†æœ‰ä¸åŒçš„å°ºåº¦ï¼Œä¸åŒ¹é…ç‹—çš„è¾¹ç•Œæ¡†ï¼Œå› æ­¤åœ¨è®­ç»ƒæœŸé—´è¢«è®¤ä¸ºæ˜¯è´Ÿä¾‹ã€‚**

```Hard negative mining``` After the matching step, most of the default boxes are nega- tives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training.

```éš¾ä¾‹æŒ–æ˜```ã€‚åœ¨åŒ¹é…æ­¥éª¤ä¹‹åï¼Œå¤§å¤šæ•°é»˜è®¤è¾¹ç•Œæ¡†ä¸ºè´Ÿä¾‹ï¼Œå°¤å…¶æ˜¯å½“å¯èƒ½çš„é»˜è®¤è¾¹ç•Œæ¡†æ•°é‡è¾ƒå¤šæ—¶ã€‚è¿™åœ¨æ­£çš„è®­ç»ƒå®ä¾‹å’Œè´Ÿçš„è®­ç»ƒå®ä¾‹ä¹‹é—´å¼•å…¥äº†æ˜¾è‘—çš„ä¸å¹³è¡¡ã€‚æˆ‘ä»¬ä¸ä½¿ç”¨æ‰€æœ‰è´Ÿä¾‹ï¼Œè€Œæ˜¯ä½¿ç”¨æ¯ä¸ªé»˜è®¤è¾¹ç•Œæ¡†çš„æœ€é«˜ç½®ä¿¡åº¦æŸå¤±æ¥æ’åºå®ƒä»¬ï¼Œå¹¶æŒ‘é€‰æœ€é«˜çš„ç½®ä¿¡åº¦ï¼Œä»¥ä¾¿è´Ÿä¾‹å’Œæ­£ä¾‹ä¹‹é—´çš„æ¯”ä¾‹è‡³å¤šä¸º3:1ã€‚æˆ‘ä»¬å‘ç°è¿™ä¼šå¯¼è‡´æ›´å¿«çš„ä¼˜åŒ–å’Œæ›´ç¨³å®šçš„è®­ç»ƒã€‚

```Data augmentation``` To make the model more robust to various input object sizes and shapes, each training image is randomly sampled by one of the following options:
**æ•°æ®å¢å¼ºã€‚ä¸ºäº†ä½¿æ¨¡å‹å¯¹å„ç§è¾“å…¥ç›®æ ‡å¤§å°å’Œå½¢çŠ¶æ›´é²æ£’ï¼Œæ¯å¼ è®­ç»ƒå›¾åƒéƒ½æ˜¯é€šè¿‡ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€è¿›è¡Œéšæœºé‡‡æ ·çš„ï¼š**

 - Use the entire original input image.
 - **ä½¿ç”¨æ•´ä¸ªåŸå§‹è¾“å…¥å›¾åƒã€‚**
 - Sample a patch so that the minimum jaccard overlap with the objects is 0.1, 0.3,0.5, 0.7, or 0.9.
 - **é‡‡æ ·ä¸€ä¸ªå›¾åƒå—ï¼Œä½¿å¾—ä¸ç›®æ ‡ä¹‹é—´çš„æœ€å°Jaccardé‡å ä¸º0.1ï¼Œ0.3ï¼Œ0.5ï¼Œ0.7æˆ–0.9ã€‚**
 - Randomly sample a patch.
 - **éšæœºé‡‡æ ·ä¸€ä¸ªå›¾åƒå—ã€‚**

The size of each sampled patch is [0.1, 1] of the original image size, and the aspect ratio is between $\frac {1} {2}$ and 2. We keep the overlapped part of the ground truth box if the center of 2 it is in the sampled patch. After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].

**æ¯ä¸ªé‡‡æ ·å›¾åƒå—çš„å¤§å°æ˜¯åŸå§‹å›¾åƒå¤§å°çš„[0.1ï¼Œ1]ï¼Œé•¿å®½æ¯”åœ¨$\frac {1} {2}$å’Œ2ä¹‹é—´ã€‚å¦‚æœå®é™…è¾¹ç•Œæ¡†çš„ä¸­å¿ƒåœ¨é‡‡ç”¨çš„å›¾åƒå—ä¸­ï¼Œæˆ‘ä»¬ä¿ç•™å®é™…è¾¹ç•Œæ¡†ä¸é‡‡æ ·å›¾åƒå—çš„é‡å éƒ¨åˆ†ã€‚åœ¨ä¸Šè¿°é‡‡æ ·æ­¥éª¤ä¹‹åï¼Œé™¤äº†åº”ç”¨ç±»ä¼¼äºæ–‡çŒ®[14]ä¸­æè¿°çš„ä¸€äº›å…‰åº¦å˜å½¢ä¹‹å¤–ï¼Œå°†æ¯ä¸ªé‡‡æ ·å›¾åƒå—è°ƒæ•´åˆ°å›ºå®šå°ºå¯¸å¹¶ä»¥0.5çš„æ¦‚ç‡è¿›è¡Œæ°´å¹³ç¿»è½¬ã€‚**

### 3 Experimental Results
#### 3. å®éªŒç»“æœ

```Base network``` Our experiments are all based on VGG16 [15], which is pre-trained on the ILSVRC CLS-LOC dataset [16]. Similar to DeepLab-LargeFOV [17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from$2Ã—2âˆ’s2$ to $3Ã—3âˆ’s1$,andusethea` trousalgorithm[18]tofillthe â€holesâ€. We remove all the dropout layers and the fc8 layer. We fine-tune the resulting model using SGD with initial learning rate $10^{âˆ’3}$, 0.9 momentum, 0.0005 weight decay, and batch size 32. The learning rate decay policy is slightly different for each dataset, and we will describe details later. The full training and testing code is built on Caffe [19] and is open source at: https://github.com/weiliu89/caffe/tree/ssd .

**```åŸºç¡€ç½‘ç»œ```ã€‚æˆ‘ä»¬çš„å®éªŒå…¨éƒ¨åŸºäºVGG16[15]ï¼Œå®ƒæ˜¯åœ¨ILSVRC CLS-LOCæ•°æ®é›†[16]ä¸Šé¢„å…ˆè®­ç»ƒçš„ã€‚ç±»ä¼¼äºDeepLab-LargeFOV[17]ï¼Œæˆ‘ä»¬å°†fc6å’Œfc7è½¬æ¢ä¸ºå·ç§¯å±‚ï¼Œä»fc6å’Œfc7ä¸­é‡é‡‡æ ·å‚æ•°ï¼Œå°†pool5ä»$2Ã—2âˆ’s2$æ›´æ”¹ä¸º$3Ã—3âˆ’s1$ï¼Œå¹¶ä½¿ç”¨ç©ºæ´ç®—æ³•[18]æ¥å¡«è¡¥è¿™ä¸ªâ€œå°æ´â€ã€‚æˆ‘ä»¬åˆ é™¤æ‰€æœ‰çš„ä¸¢å¼ƒå±‚å’Œfc8å±‚ã€‚æˆ‘ä»¬ä½¿ç”¨SGDå¯¹å¾—åˆ°çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º$10^{âˆ’3}ï¼ŒåŠ¨é‡ä¸º0.9ï¼Œæƒé‡è¡°å‡ä¸º0.0005ï¼Œæ‰¹æ•°æ®å¤§å°ä¸º32ã€‚æ¯ä¸ªæ•°æ®é›†çš„å­¦ä¹ é€Ÿç‡è¡°å‡ç­–ç•¥ç•¥æœ‰ä¸åŒï¼Œæˆ‘ä»¬å°†åœ¨åé¢è¯¦ç»†æè¿°ã€‚å®Œæ•´çš„è®­ç»ƒå’Œæµ‹è¯•ä»£ç å»ºç«‹åœ¨Caffe[19]ä¸Šå¹¶å¼€æºï¼šhttps://github.com/weiliu89/caffe/tree/ssdã€‚**


----------

#### 3.1 PASCAL VOC2007
#### 3.1 PASCAL VOC2007

On this dataset, we compare against Fast R-CNN [6] and Faster R-CNN [2] on VOC2007 test (4952 images). All methods fine-tune on the same pre-trained VGG16 network.

**åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬åœ¨VOC2007 testï¼ˆ4952å¼ å›¾åƒï¼‰ä¸Šæ¯”è¾ƒäº†Fast R-CNN[6]å’ŒFAST R-CNN[2]ã€‚æ‰€æœ‰çš„æ–¹æ³•éƒ½åœ¨ç›¸åŒçš„é¢„è®­ç»ƒå¥½çš„VGG16ç½‘ç»œä¸Šè¿›è¡Œå¾®è°ƒã€‚**

Figure 2 shows the architecture details of the SSD300 model. We use conv4 3, conv7 (fc7), conv8 2, conv9 2, conv10 2, and conv11 2 to predict both location and confidences. We set default box with scale 0.1 on conv4 33. We initialize the parameters for all the newly added convolutional layers with the â€xavierâ€ method [20]. For conv4 3, conv10 2 and conv11 2, we only associate 4 default boxes at each feature map locationâ€”â€”omitting aspect ratios of $\frac {1} {3}$ and 3. For all other layers, we put 6 default boxes as 3 described in Sec. 2.2. Since, as pointed out in [12], conv4 3 has a different feature scale compared to the other layers, we use the L2 normalization technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back propagation. We use the 10âˆ’3 learning rate for 40k iterations, then continue training for 10k iterations with 10âˆ’4 and 10âˆ’5. When training on VOC2007 trainval, Table 1 shows that our low resolution SSD300 model is already more accurate than Fast R-CNN. When we train SSD on a larger 512 Ã— 512 input image, it is even more accurate, surpassing Faster R-CNN by 1.7% mAP. If we train SSD with more (i.e. 07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1% and that SSD512 is 3.6% better. If we take models trained on COCO trainval35k as described in Sec. 3.4 and fine-tuning them on the 07+12 dataset with SSD512, we achieve the best results: 81.6% mAP.

**å›¾2æ˜¾ç¤ºäº†SSD300æ¨¡å‹çš„æ¶æ„ç»†èŠ‚ã€‚æˆ‘ä»¬ä½¿ç”¨conv4_3ï¼Œconv7ï¼ˆfc7ï¼‰ï¼Œconv8_2ï¼Œconv9_2ï¼Œconv10_2å’Œconv11_2æ¥é¢„æµ‹ä½ç½®å’Œç½®ä¿¡åº¦ã€‚æˆ‘ä»¬åœ¨conv4_3ä¸Šè®¾ç½®äº†å°ºåº¦ä¸º0.1çš„é»˜è®¤è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬ä½¿ç”¨â€œxavierâ€æ–¹æ³•[20]åˆå§‹åŒ–æ‰€æœ‰æ–°æ·»åŠ çš„å·ç§¯å±‚çš„å‚æ•°ã€‚å¯¹äºconv4_3ï¼Œconv10_2å’Œconv11_2ï¼Œæˆ‘ä»¬åªåœ¨æ¯ä¸ªç‰¹å¾æ˜ å°„ä½ç½®ä¸Šå…³è”äº†4ä¸ªé»˜è®¤è¾¹ç•Œæ¡†â€”â€”å¿½ç•¥$\frac {1} {3}$å’Œ3çš„é•¿å®½æ¯”ã€‚å¯¹äºæ‰€æœ‰å…¶å®ƒå±‚ï¼Œæˆ‘ä»¬åƒ2.2èŠ‚æè¿°çš„é‚£æ ·æ”¾ç½®äº†6ä¸ªé»˜è®¤è¾¹ç•Œæ¡†ã€‚å¦‚[12]æ‰€æŒ‡å‡ºçš„ï¼Œä¸å…¶å®ƒå±‚ç›¸æ¯”ï¼Œç”±äºconv4_3å…·æœ‰ä¸åŒçš„ç‰¹å¾å°ºåº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨[12]ä¸­å¼•å…¥çš„L2æ­£åˆ™åŒ–æŠ€æœ¯å°†ç‰¹å¾æ˜ å°„ä¸­æ¯ä¸ªä½ç½®çš„ç‰¹å¾æ ‡å‡†ç¼©æ”¾åˆ°20ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å­¦ä¹ å°ºåº¦ã€‚å¯¹äº40kæ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬ä½¿ç”¨$10^{âˆ’3}$çš„å­¦ä¹ ç‡ï¼Œç„¶åç»§ç»­ç”¨$10^{âˆ’4}$å’Œ$10^{âˆ’5}$çš„å­¦ä¹ ç‡è®­ç»ƒ10kè¿­ä»£ã€‚å½“å¯¹VOC2007 ğšğš›ğšŠğš’ğš—ğšŸğšŠğš•è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¡¨1æ˜¾ç¤ºäº†æˆ‘ä»¬çš„ä½åˆ†è¾¨ç‡SSD300æ¨¡å‹å·²ç»æ¯”Fast R-CNNæ›´å‡†ç¡®ã€‚å½“æˆ‘ä»¬ç”¨æ›´å¤§çš„512Ã—512è¾“å…¥å›¾åƒä¸Šè®­ç»ƒSSDæ—¶ï¼Œå®ƒæ›´åŠ å‡†ç¡®ï¼Œè¶…è¿‡äº†Faster R-CNN $1.7\%$çš„mAPã€‚å¦‚æœæˆ‘ä»¬ç”¨æ›´å¤šçš„ï¼ˆå³07+12ï¼‰æ•°æ®æ¥è®­ç»ƒSSDï¼Œæˆ‘ä»¬çœ‹åˆ°SSD300å·²ç»æ¯”Faster R-CNNå¥½$1.1\%$ï¼ŒSSD512æ¯”Faster R-CNNå¥½$3.6\%$ã€‚å¦‚æœæˆ‘ä»¬å°†SSD512ç”¨3.4èŠ‚æè¿°çš„COCO ğšğš›ğšŠğš’ğš—ğšŸğšŠğš•ğŸ¹ğŸ»ğš”æ¥è®­ç»ƒæ¨¡å‹å¹¶åœ¨07+12æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬è·å¾—äº†æœ€å¥½çš„ç»“æœï¼š$81.6\%$çš„mAPã€‚**

![3](/images/posts/ssd/3.png)

Table 1: PASCAL VOC2007 test detection results. Both Fast and Faster R-CNN use input images whose minimum dimension is 600. The two SSD models have exactly the same settings except that they have different input sizes (300Ã—300 vs. 512Ã—512). It is obvious that larger input size leads to better results, and more data always helps. Data: â€07â€: VOC2007 trainval, â€07+12â€: union of VOC2007 and VOC2012 trainval. â€07+12+COCOâ€: first train on COCO trainval35k then fine-tune on 07+12.

**è¡¨1ï¼šPASCAL VOC2007 ``testæ£€æµ‹ç»“æœã€‚Fastå’ŒFaster R-CNNéƒ½ä½¿ç”¨æœ€å°ç»´åº¦ä¸º600çš„è¾“å…¥å›¾åƒã€‚ä¸¤ä¸ªSSDæ¨¡å‹ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è®¾ç½®é™¤äº†å®ƒä»¬æœ‰ä¸åŒçš„è¾“å…¥å¤§å°(300Ã—300å’Œ512Ã—512)ã€‚å¾ˆæ˜æ˜¾æ›´å¤§çš„è¾“å…¥å°ºå¯¸ä¼šå¯¼è‡´æ›´å¥½çš„ç»“æœï¼Œå¹¶ä¸”æ›´å¤§çš„æ•°æ®åŒæ ·æœ‰å¸®åŠ©ã€‚æ•°æ®ï¼šâ€œ07â€ï¼šVOC2007 ```trainval```ï¼Œâ€œ07+12â€ï¼šVOC2007å’ŒVOC2012 ```trainval```çš„è”åˆã€‚â€œ07+12+COCOâ€ï¼šé¦–å…ˆåœ¨COCO trainval35kä¸Šè®­ç»ƒç„¶ååœ¨07+12ä¸Šå¾®è°ƒã€‚**

To understand the performance of our two SSD models in more details, we used the detection analysis tool from [21]. Figure 3 shows that SSD can detect various object categories with high quality (large white area). The majority of its confident detections are correct. The recall is around 85-90%, and is much higher with â€œweakâ€ (0.1 jaccard overlap) criteria. Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two decoupled steps. However, SSD has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories. Figure 4 shows that SSD is very sensitive to the bounding box size. In other words, it has much worse performance on smaller objects than bigger objects. This is not surprising because those small objects may not even have any information at the very top layers. Increasing the input size (e.g. from 300 Ã— 300 to 512 Ã— 512) can help improve detecting small objects, but there is still a lot of room to improve. On the positive side, we can clearly see that SSD performs really well on large objects. And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location.

**ä¸ºäº†æ›´è¯¦ç»†åœ°äº†è§£æˆ‘ä»¬ä¸¤ä¸ªSSDæ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†[21]ä¸­çš„æ£€æµ‹åˆ†æå·¥å…·ã€‚å›¾3æ˜¾ç¤ºäº†SSDå¯ä»¥æ£€æµ‹åˆ°é«˜è´¨é‡ï¼ˆå¤§ç™½è‰²åŒºåŸŸï¼‰çš„å„ç§ç›®æ ‡ç±»åˆ«ã€‚å®ƒå¤§éƒ¨åˆ†çš„ç¡®ä¿¡æ£€æµ‹æ˜¯æ­£ç¡®çš„ã€‚å¬å›çº¦ä¸º85âˆ’90%ï¼Œè€Œâ€œå¼±â€ï¼ˆ0.1 Jaccardé‡å ï¼‰æ ‡å‡†åˆ™è¦é«˜å¾—å¤šã€‚ä¸R-CNN[22]ç›¸æ¯”ï¼ŒSSDå…·æœ‰æ›´å°çš„å®šä½è¯¯å·®ï¼Œè¡¨æ˜SSDå¯ä»¥æ›´å¥½åœ°å®šä½ç›®æ ‡ï¼Œå› ä¸ºå®ƒç›´æ¥å­¦ä¹ å›å½’ç›®æ ‡å½¢çŠ¶å’Œåˆ†ç±»ç›®æ ‡ç±»åˆ«ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¸¤ä¸ªè§£è€¦æ­¥éª¤ã€‚ç„¶è€Œï¼ŒSSDå¯¹ç±»ä¼¼çš„ç›®æ ‡ç±»åˆ«ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºåŠ¨ç‰©ï¼‰æœ‰æ›´å¤šçš„æ··æ·†ï¼Œéƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬å…±äº«å¤šä¸ªç±»åˆ«çš„ä½ç½®ã€‚å›¾4æ˜¾ç¤ºSSDå¯¹è¾¹ç•Œæ¡†å¤§å°éå¸¸æ•æ„Ÿã€‚æ¢å¥è¯è¯´ï¼Œå®ƒåœ¨è¾ƒå°ç›®æ ‡ä¸Šæ¯”åœ¨è¾ƒå¤§ç›®æ ‡ä¸Šçš„æ€§èƒ½è¦å·®å¾—å¤šã€‚è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºè¿™äº›å°ç›®æ ‡ç”šè‡³å¯èƒ½åœ¨é¡¶å±‚æ²¡æœ‰ä»»ä½•ä¿¡æ¯ã€‚å¢åŠ è¾“å…¥å°ºå¯¸ï¼ˆä¾‹å¦‚ä»300Ã—300åˆ°512Ã—512ï¼‰å¯ä»¥å¸®åŠ©æ”¹è¿›æ£€æµ‹å°ç›®æ ‡ï¼Œä½†ä»ç„¶æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ç§¯æçš„ä¸€é¢ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°SSDåœ¨å¤§å‹ç›®æ ‡ä¸Šçš„è¡¨ç°éå¸¸å¥½ã€‚è€Œä¸”å¯¹äºä¸åŒé•¿å®½æ¯”çš„ç›®æ ‡ï¼Œå®ƒæ˜¯éå¸¸é²æ£’çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨æ¯ä¸ªç‰¹å¾æ˜ å°„ä½ç½®çš„å„ç§é•¿å®½æ¯”çš„é»˜è®¤æ¡†ã€‚**

![4](/images/posts/ssd/4.png)

Fig. 3: Visualization of performance for SSD512 on animals, vehicles, and furni- ture from VOC2007 test. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line reflects the change of recall with strong criteria (0.5 jaccard overlap) as the num- ber of detections increases. The dashed red line is using the weak criteria (0.1 jaccard overlap). The bottom row shows the distribution of top-ranked false positive types.

**å›¾3ï¼šSSD512åœ¨VOC2007 testä¸­çš„åŠ¨ç‰©ï¼Œè½¦è¾†å’Œå®¶å…·ä¸Šçš„æ€§èƒ½å¯è§†åŒ–ã€‚ç¬¬ä¸€è¡Œæ˜¾ç¤ºç”±äºå®šä½ä¸ä½³ï¼ˆLocï¼‰ï¼Œä¸ç›¸ä¼¼ç±»åˆ«ï¼ˆSimï¼‰æ··æ·†ï¼Œä¸å…¶å®ƒï¼ˆOthï¼‰æˆ–èƒŒæ™¯ï¼ˆBGï¼‰ç›¸å…³çš„æ­£ç¡®æ£€æµ‹ï¼ˆCorï¼‰æˆ–å‡é˜³æ€§çš„ç´¯ç§¯åˆ†æ•°ã€‚çº¢è‰²çš„å®çº¿è¡¨ç¤ºéšç€æ£€æµ‹æ¬¡æ•°çš„å¢åŠ ï¼Œå¼ºæ ‡å‡†ï¼ˆ0.5 Jaccardé‡å ï¼‰ä¸‹çš„å¬å›å˜åŒ–ã€‚çº¢è‰²è™šçº¿æ˜¯ä½¿ç”¨å¼±æ ‡å‡†ï¼ˆ0.1 Jaccardé‡å ï¼‰ã€‚æœ€ä¸‹é¢ä¸€è¡Œæ˜¾ç¤ºäº†æ’åé å‰çš„å‡é˜³æ€§ç±»å‹çš„åˆ†å¸ƒã€‚**

![5](/images/posts/ssd/5.png)

Fig. 4: Sensitivity and impact of different object characteristics on VOC2007 test set using [21]. The plot on the left shows the effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small; S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW =extra-wide.

**å›¾4ï¼šä½¿ç”¨[21]åœ¨VOC2007 testè®¾ç½®ä¸Šä¸åŒç›®æ ‡ç‰¹æ€§çš„çµæ•åº¦å’Œå½±å“ã€‚å·¦è¾¹çš„å›¾æ˜¾ç¤ºäº†BBoxé¢ç§¯å¯¹æ¯ä¸ªç±»åˆ«çš„å½±å“ï¼Œå³è¾¹çš„å›¾æ˜¾ç¤ºäº†é•¿å®½æ¯”çš„å½±å“ã€‚å…³é”®ï¼šBBoxåŒºåŸŸï¼šXS=è¶…å°ï¼›S=å°ï¼›M=ä¸­ç­‰ï¼›L=å¤§ï¼›XL=è¶…å¤§ã€‚é•¿å®½æ¯”ï¼šXT=è¶…é«˜/çª„ï¼›T=é«˜ï¼›M=ä¸­ç­‰ï¼›W=å®½ï¼›XW =è¶…å®½ã€‚**


----------

#### 3.2 Model analysis
#### 3.2 æ¨¡å‹åˆ†æ

To understand SSD better, we carried out controlled experiments to examine how each component affects performance. For all the experiments, we use the same settings and input size (300 Ã— 300), except for specified changes to the settings or component(s).

**ä¸ºäº†æ›´å¥½åœ°äº†è§£SSDï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ§åˆ¶å®éªŒï¼Œä»¥æ£€æŸ¥æ¯ä¸ªç»„ä»¶å¦‚ä½•å½±å“æ€§èƒ½ã€‚å¯¹äºæ‰€æœ‰çš„å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„è®¾ç½®å’Œè¾“å…¥å¤§å°ï¼ˆ300Ã—300ï¼‰ï¼Œé™¤äº†æŒ‡å®šçš„è®¾ç½®æˆ–ç»„ä»¶çš„æ›´æ”¹ã€‚**

![6](/images/posts/ssd/6.jpg)

**è¡¨2ï¼šå„ç§è®¾è®¡é€‰æ‹©å’Œç»„ä»¶å¯¹SSDæ€§èƒ½çš„å½±å“ã€‚**

```Data augmentation is crucial```. Fast and Faster R-CNN use the original image and the horizontal flip to train. We use a more extensive sampling strategy, similar to YOLO [5]. Table 2 shows that we can improve 8.8% mAP with this sampling strategy. We do not know how much our sampling strategy will benefit Fast and Faster R-CNN, but they are likely to benefit less because they use a feature pooling step during classification that is relatively robust to object translation by design.

**```æ•°æ®å¢å¼ºè‡³å…³é‡è¦```ã€‚Fastå’ŒFaster R-CNNä½¿ç”¨åŸå§‹å›¾åƒå’Œæ°´å¹³ç¿»è½¬æ¥è®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨æ›´å¹¿æ³›çš„æŠ½æ ·ç­–ç•¥ï¼Œç±»ä¼¼äºYOLO[5]ã€‚ä»è¡¨2å¯ä»¥çœ‹å‡ºï¼Œé‡‡æ ·ç­–ç•¥å¯ä»¥æé«˜8.8%çš„mAPã€‚æˆ‘ä»¬ä¸çŸ¥é“æˆ‘ä»¬çš„é‡‡æ ·ç­–ç•¥å°†ä¼šä½¿Fastå’ŒFaster R-CNNå—ç›Šå¤šå°‘ï¼Œä½†æ˜¯ä»–ä»¬å¯èƒ½ä»ä¸­å—ç›Šè¾ƒå°‘ï¼Œå› ä¸ºä»–ä»¬åœ¨åˆ†ç±»è¿‡ç¨‹ä¸­ä½¿ç”¨äº†ä¸€ä¸ªç‰¹å¾æ± åŒ–æ­¥éª¤ï¼Œè¿™å¯¹é€šè¿‡è®¾è®¡çš„ç›®æ ‡å˜æ¢æ¥è¯´ç›¸å¯¹é²æ£’ã€‚**

More default box shapes is better. As described in Sec. 2.2, by default we use 6 default boxes per location. If we remove the boxes with $\frac13$ and 3 aspect ratios, the performance drops by 0.6%. By further removing the boxes with $\frac12$ and 2 aspect ratios, the performance drops another 2.1%. Using a variety of default box shapes seems to make the task of predicting boxes easier for the network.

**```æ›´å¤šçš„é»˜è®¤è¾¹ç•Œæ¡†å½¢çŠ¶ä¼šæ›´å¥½```ã€‚å¦‚2.2èŠ‚æ‰€è¿°ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ¯ä¸ªä½ç½®ä½¿ç”¨6ä¸ªé»˜è®¤è¾¹ç•Œæ¡†ã€‚å¦‚æœæˆ‘ä»¬åˆ é™¤é•¿å®½æ¯”ä¸º$\frac {1} {3}$å’Œ3çš„è¾¹ç•Œæ¡†ï¼Œæ€§èƒ½ä¸‹é™äº†0.6%ã€‚é€šè¿‡è¿›ä¸€æ­¥å»é™¤$\frac {1} {2}$å’Œ2é•¿å®½æ¯”çš„ç›’å­ï¼Œæ€§èƒ½å†ä¸‹é™2.1%ã€‚ä½¿ç”¨å„ç§é»˜è®¤è¾¹ç•Œæ¡†å½¢çŠ¶ä¼¼ä¹ä½¿ç½‘ç»œé¢„æµ‹è¾¹ç•Œæ¡†çš„ä»»åŠ¡æ›´å®¹æ˜“ã€‚**

```Atrous is faster```. As described in Sec. 3, we used the atrous version of a subsampled VGG16, following DeepLab-LargeFOV [17]. If we use the full VGG16, keeping pool5 with 2 Ã— 2 âˆ’ s2 and not subsampling parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about 20% slower.

**```Atrousæ›´å¿«```ã€‚å¦‚ç¬¬3èŠ‚æ‰€è¿°ï¼Œæˆ‘ä»¬æ ¹æ®DeepLab-LargeFOV[17]ä½¿ç”¨å­é‡‡æ ·çš„VGG16çš„ç©ºæ´ç‰ˆæœ¬ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„VGG16ï¼Œä¿æŒpool5ä¸º2Ã—2-s2ï¼Œå¹¶ä¸”ä¸ä»fc6å’Œfc7ä¸­å­é‡‡æ ·å‚æ•°ï¼Œå¹¶æ·»åŠ conv5_3è¿›è¡Œé¢„æµ‹ï¼Œç»“æœå¤§è‡´ç›¸åŒï¼Œè€Œé€Ÿåº¦æ…¢äº†å¤§çº¦20%ã€‚**

Multiple output layers at different resolutions is better. A major contribution of SSD is using default boxes of different scales on different output layers. To measure the advantage gained, we progressively remove layers and compare results. For a fair comparison, every time we remove a layer, we adjust the default box tiling to keep the total number of boxes similar to the original (8732). This is done by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed. We do not exhaustively optimize the tiling for each setting. Table 3 shows a decrease in accuracy with fewer layers, dropping monotonically from 74.3 to 62.4. When we stack boxes of multiple scales on a layer, many are on the image boundary and need to be handled carefully. We tried the strategy used in Faster R-CNN [2], ignoring boxes which are on the boundary. We observe some interesting trends. For example, it hurts the performance by a large margin if we use very coarse feature maps (e.g. conv11 2 (1 Ã— 1) or conv10 2 (3 Ã— 3)). The reason might be that we do not have enough large boxes to cover large objects after the pruning. When we use primarily finer resolution maps, the performance starts increasing again because even after pruning a sufficient number of large boxes remains. If we only use conv7 for prediction, the performance is the worst, reinforcing the message that it is critical to spread boxes of different scales over dif- ferent layers. Besides, since our predictions do not rely on ROI pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23]. The SSD architecture combines predictions from feature maps of various resolutions to achieve comparable accuracy to Faster R-CNN, while using lower resolution input images.

**```å¤šä¸ªä¸åŒåˆ†è¾¨ç‡çš„è¾“å‡ºå±‚æ›´å¥½```ã€‚SSDçš„ä¸»è¦è´¡çŒ®æ˜¯åœ¨ä¸åŒçš„è¾“å‡ºå±‚ä¸Šä½¿ç”¨ä¸åŒå°ºåº¦çš„é»˜è®¤è¾¹ç•Œæ¡†ã€‚ä¸ºäº†è¡¡é‡æ‰€è·å¾—çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬é€æ­¥åˆ é™¤å±‚å¹¶æ¯”è¾ƒç»“æœã€‚ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæ¯æ¬¡æˆ‘ä»¬åˆ é™¤ä¸€å±‚ï¼Œæˆ‘ä»¬è°ƒæ•´é»˜è®¤è¾¹ç•Œæ¡†å¹³é“ºï¼Œä»¥ä¿æŒç±»ä¼¼äºæœ€åˆçš„è¾¹ç•Œæ¡†çš„æ€»æ•°ï¼ˆ8732ï¼‰ã€‚è¿™æ˜¯é€šè¿‡åœ¨å‰©ä½™å±‚ä¸Šå †å æ›´å¤šå°ºåº¦çš„ç›’å­å¹¶æ ¹æ®éœ€è¦è°ƒæ•´è¾¹ç•Œæ¡†çš„å°ºåº¦æ¥å®Œæˆçš„ã€‚æˆ‘ä»¬æ²¡æœ‰è¯¦å°½åœ°ä¼˜åŒ–æ¯ä¸ªè®¾ç½®çš„å¹³é“ºã€‚è¡¨3æ˜¾ç¤ºå±‚æ•°è¾ƒå°‘ï¼Œç²¾åº¦é™ä½ï¼Œä»74.3å•è°ƒé€’å‡è‡³62.4ã€‚å½“æˆ‘ä»¬åœ¨ä¸€å±‚ä¸Šå †å å¤šå°ºåº¦çš„è¾¹ç•Œæ¡†æ—¶ï¼Œå¾ˆå¤šè¾¹ç•Œæ¡†åœ¨å›¾åƒè¾¹ç•Œä¸Šéœ€è¦å°å¿ƒå¤„ç†ã€‚æˆ‘ä»¬å°è¯•äº†åœ¨Faster R-CNN[2]ä¸­ä½¿ç”¨è¿™ä¸ªç­–ç•¥ï¼Œå¿½ç•¥åœ¨è¾¹ç•Œä¸Šçš„è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€äº›æœ‰è¶£çš„è¶‹åŠ¿ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨éå¸¸ç²—ç³™çš„ç‰¹å¾æ˜ å°„ï¼ˆä¾‹å¦‚conv11_2ï¼ˆ1Ã—1ï¼‰æˆ–conv10_2ï¼ˆ3Ã—3ï¼‰ï¼‰ï¼Œå®ƒä¼šå¤§å¤§ä¼¤å®³æ€§èƒ½ã€‚åŸå› å¯èƒ½æ˜¯ä¿®å‰ªåæˆ‘ä»¬æ²¡æœ‰è¶³å¤Ÿå¤§çš„è¾¹ç•Œæ¡†æ¥è¦†ç›–å¤§çš„ç›®æ ‡ã€‚å½“æˆ‘ä»¬ä¸»è¦ä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„ç‰¹å¾æ˜ å°„æ—¶ï¼Œæ€§èƒ½å¼€å§‹å†æ¬¡ä¸Šå‡ï¼Œå› ä¸ºå³ä½¿åœ¨ä¿®å‰ªä¹‹åä»ç„¶æœ‰è¶³å¤Ÿæ•°é‡çš„å¤§è¾¹ç•Œæ¡†ã€‚å¦‚æœæˆ‘ä»¬åªä½¿ç”¨conv7è¿›è¡Œé¢„æµ‹ï¼Œé‚£ä¹ˆæ€§èƒ½æ˜¯æœ€ç³Ÿç³•çš„ï¼Œè¿™å°±å¼ºåŒ–äº†```åœ¨ä¸åŒå±‚ä¸Šæ‰©å±•ä¸åŒå°ºåº¦çš„è¾¹ç•Œæ¡†æ˜¯éå¸¸å…³é”®çš„```ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„é¢„æµ‹ä¸åƒ[6]é‚£æ ·ä¾èµ–äºROIæ± åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ä½åˆ†è¾¨ç‡ç‰¹å¾æ˜ å°„ä¸­æ²¡æœ‰æŠ˜å ç»„å—çš„é—®é¢˜[23]ã€‚SSDæ¶æ„å°†æ¥è‡ªå„ç§åˆ†è¾¨ç‡çš„ç‰¹å¾æ˜ å°„çš„é¢„æµ‹ç»“åˆèµ·æ¥ï¼Œä»¥è¾¾åˆ°ä¸Faster R-CNNç›¸å½“çš„ç²¾ç¡®åº¦ï¼ŒåŒæ—¶ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡çš„è¾“å…¥å›¾åƒã€‚**

![7](/images/posts/ssd/7.jpg)

**è¡¨3ï¼šä½¿ç”¨å¤šä¸ªè¾“å‡ºå±‚çš„å½±å“ã€‚**


----------
#### 3.3 PASCAL VOC2012
#### 3.3 PASCAL VOC2012

We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images). We train the models with 10âˆ’3 learning rate for 60k iterations, then 10âˆ’4 for 20k iterations. Table 4 shows the results of our SSD300 and SSD5124 model. We see the same performance trend as we observed on VOC2007 test. Our SSD300 improves accuracy over Fast/Faster R- CNN. By increasing the training and testing image size to 512 Ã— 512, we are 4.5% more accurate than Faster R-CNN. Compared to YOLO, SSD is significantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training. When fine-tuned from models trained on COCO, our SSD512 achieves 80.0% mAP, which is 4.1% higher than Faster R-CNN.

**é™¤äº†æˆ‘ä»¬ä½¿ç”¨VOC2012 trainvalå’ŒVOC2007 trainvalï¼Œtestï¼ˆ21503å¼ å›¾åƒï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠåœ¨VOC2012 testï¼ˆ10991å¼ å›¾åƒï¼‰ä¸Šè¿›è¡Œæµ‹è¯•ä¹‹å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ä¸Šè¿°åŸºæœ¬çš„VOC2007å®éªŒç›¸åŒçš„è®¾ç½®ã€‚æˆ‘ä»¬ç”¨$10^{âˆ’3}$çš„å­¦ä¹ ç‡å¯¹æ¨¡å‹è¿›è¡Œ60kæ¬¡çš„è¿­ä»£è®­ç»ƒï¼Œç„¶åä½¿ç”¨$10^{âˆ’4}$çš„å­¦ä¹ ç‡è¿›è¡Œ20kæ¬¡è¿­ä»£è®­ç»ƒã€‚è¡¨4æ˜¾ç¤ºäº†æˆ‘ä»¬çš„SSD300å’ŒSSD512æ¨¡å‹çš„ç»“æœã€‚æˆ‘ä»¬çœ‹åˆ°äº†ä¸æˆ‘ä»¬åœ¨VOC2007 testä¸­è§‚å¯Ÿåˆ°çš„ç›¸åŒçš„æ€§èƒ½è¶‹åŠ¿ã€‚æˆ‘ä»¬çš„SSD300æ¯”Fast/Faster R-CNNæé«˜äº†å‡†ç¡®æ€§ã€‚é€šè¿‡å°†è®­ç»ƒå’Œæµ‹è¯•å›¾åƒå¤§å°å¢åŠ åˆ°512Ã—512ï¼Œæˆ‘ä»¬æ¯”Faster R-CNNçš„å‡†ç¡®ç‡æé«˜äº†4.5%ã€‚ä¸YOLOç›¸æ¯”ï¼ŒSSDæ›´ç²¾ç¡®ï¼Œå¯èƒ½æ˜¯ç”±äºä½¿ç”¨äº†æ¥è‡ªå¤šä¸ªç‰¹å¾æ˜ å°„çš„å·ç§¯é»˜è®¤è¾¹ç•Œæ¡†å’Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´çš„åŒ¹é…ç­–ç•¥ã€‚å½“å¯¹ä»COCOä¸Šè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒåï¼Œæˆ‘ä»¬çš„SSD512è¾¾åˆ°äº†80.0%çš„mAPï¼Œæ¯”Faster R-CNNé«˜äº†4.1%ã€‚**

![8](/images/posts/ssd/8.png)

Table 4: PASCAL VOC2012 test detection results. Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 Ã— 448. data: â€07++12â€: union of VOC2007 trainval and test and VOC2012 trainval. â€07++12+COCOâ€: first train on COCO trainval35k then fine-tune on 07++12.
**è¡¨4ï¼š PASCAL VOC2012 testä¸Šçš„æ£€æµ‹ç»“æœ. Fastå’ŒFaster R-CNNä½¿ç”¨æœ€å°ç»´åº¦ä¸º600çš„å›¾åƒï¼Œè€ŒYOLOçš„å›¾åƒå¤§å°ä¸º448Ã— 448ã€‚æ•°æ®ï¼šâ€œ07++12â€ï¼šVOC2007 trainvalï¼Œtestå’ŒVOC2012 trainvalã€‚â€œ07++12+COCOâ€ï¼šå…ˆåœ¨COCO trainval 35kä¸Šè®­ç»ƒç„¶ååœ¨07++12ä¸Šå¾®è°ƒã€‚**


----------

#### 3.4 COCO
To further validate the SSD framework, we trained our SSD300 and SSD512 architec- tures on the COCO dataset. Since objects in COCO tend to be smaller than PASCAL VOC, we use smaller default boxes for all layers. We follow the strategy mentioned in Sec. 2.2, but now our smallest default box has a scale of 0.15 instead of 0.2, and the scale of the default box on conv4 3 is 0.07 (e.g. 21 pixels for a 300 Ã— 300 image)5.

**ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯SSDæ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨COCOæ•°æ®é›†ä¸Šå¯¹SSD300å’ŒSSD512æ¶æ„è¿›è¡Œäº†è®­ç»ƒã€‚ç”±äºCOCOä¸­çš„ç›®æ ‡å¾€å¾€æ¯”PASCAL VOCä¸­çš„æ›´å°ï¼Œå› æ­¤æˆ‘ä»¬å¯¹æ‰€æœ‰å±‚ä½¿ç”¨è¾ƒå°çš„é»˜è®¤è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬éµå¾ª2.2èŠ‚ä¸­æåˆ°çš„ç­–ç•¥ï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬æœ€å°çš„é»˜è®¤è¾¹ç•Œæ¡†å°ºåº¦æ˜¯0.15è€Œä¸æ˜¯0.2ï¼Œå¹¶ä¸”conv4_3ä¸Šçš„é»˜è®¤è¾¹ç•Œæ¡†å°ºåº¦æ˜¯0.07ï¼ˆä¾‹å¦‚ï¼Œ300Ã—300å›¾åƒä¸­çš„21ä¸ªåƒç´ ï¼‰ã€‚**

We use the trainval35k [24] for training. We first train the model with 10âˆ’3 learning rate for 160k iterations, and then continue training for 40k iterations with 10âˆ’4 and 40k iterations with 10âˆ’5. Table 5 shows the results on test-dev2015. Similar to what we observed on the PASCAL VOC dataset, SSD300 is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95]. SSD300 has a similar mAP@0.75 as ION [24] and Faster R-CNN [25], but is worse in mAP@0.5. By increasing the im- age size to 512 Ã— 512, our SSD512 is better than Faster R-CNN [25] in both criteria. Interestingly, we observe that SSD512 is 5.3% better in mAP@0.75, but is only 1.2% better in mAP@0.5. We also observe that it has much better AP (4.8%) and AR (4.6%) for large objects, but has relatively less improvement in AP (1.3%) and AR (2.0%) for small objects. Compared to ION, the improvement in AR for large and small objects is more similar (5.4% vs. 3.9%). We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box refinement steps, in both the RPN part and in the Fast R-CNN part. In Fig. 5, we show some detection examples on COCO test-dev with the SSD512 model.

**æˆ‘ä»¬ä½¿ç”¨trainval35k[24]è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é¦–å…ˆç”¨$10^{âˆ’3}$çš„å­¦ä¹ ç‡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¿›è¡Œ160kæ¬¡è¿­ä»£ï¼Œç„¶åç»§ç»­ä»¥$10^{âˆ’4}$å’Œ$10^{âˆ’5}$çš„å­¦ä¹ ç‡å„è¿›è¡Œ40kæ¬¡è¿­ä»£ã€‚è¡¨5æ˜¾ç¤ºäº†test-dev2015çš„ç»“æœã€‚ä¸æˆ‘ä»¬åœ¨PASCAL VOCæ•°æ®é›†ä¸­è§‚å¯Ÿåˆ°çš„ç»“æœç±»ä¼¼ï¼ŒSSD300åœ¨mAP@0.5å’ŒmAP@[0.5:0.95]ä¸­éƒ½ä¼˜äºFast R-CNNã€‚SSD300ä¸ION [24]å’ŒFaster R-CNN[25]å…·æœ‰ç›¸ä¼¼çš„mAP@0.75ï¼Œä½†æ˜¯mAP@0.5æ›´å·®ã€‚é€šè¿‡å°†å›¾åƒå°ºå¯¸å¢åŠ åˆ°512Ã—512ï¼Œæˆ‘ä»¬çš„SSD512åœ¨è¿™ä¸¤ä¸ªæ ‡å‡†ä¸­éƒ½ä¼˜äºFaster R-CNN[25]ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°SSD512åœ¨mAP@0.75ä¸­è¦å¥½5.3%ï¼Œä½†æ˜¯åœ¨mAP@0.5ä¸­åªå¥½1.2%ã€‚æˆ‘ä»¬ä¹Ÿè§‚å¯Ÿåˆ°ï¼Œå¯¹äºå¤§å‹ç›®æ ‡ï¼ŒAPï¼ˆ4.8%ï¼‰å’ŒARï¼ˆ4.6%ï¼‰çš„æ•ˆæœè¦å¥½å¾—å¤šï¼Œä½†å¯¹äºå°ç›®æ ‡ï¼ŒAPï¼ˆ1.3%ï¼‰å’ŒARï¼ˆ2.0%ï¼‰æœ‰ç›¸å¯¹æ›´å°‘çš„æ”¹è¿›ã€‚ä¸IONç›¸æ¯”ï¼Œå¤§å‹å’Œå°å‹ç›®æ ‡çš„ARæ”¹è¿›æ›´ä¸ºç›¸ä¼¼ï¼ˆ5.4%å’Œ3.9%ï¼‰ã€‚æˆ‘ä»¬æ¨æµ‹Faster R-CNNåœ¨è¾ƒå°çš„ç›®æ ‡ä¸Šæ¯”SSDæ›´å…·ç«äº‰åŠ›ï¼Œå› ä¸ºå®ƒåœ¨RPNéƒ¨åˆ†å’ŒFast R-CNNéƒ¨åˆ†éƒ½æ‰§è¡Œäº†ä¸¤ä¸ªè¾¹ç•Œæ¡†ç»†åŒ–æ­¥éª¤ã€‚åœ¨å›¾5ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SSD512æ¨¡å‹åœ¨COCO test-devä¸Šçš„ä¸€äº›æ£€æµ‹å®ä¾‹ã€‚**

![9](/images/posts/ssd/9.jpg)

**è¡¨5ï¼šCOCO test-dev2015æ£€æµ‹ç»“æœã€‚**

![10](/images/posts/ssd/10.jpeg)

Fig. 5: Detection examples on COCO test-dev with SSD512 model. We show detections with scores higher than 0.6. Each color corresponds to an object category.

**å›¾5ï¼šSSD512æ¨¡å‹åœ¨COCO test-devä¸Šçš„æ£€æµ‹å®ä¾‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†åˆ†æ•°é«˜äº0.6çš„æ£€æµ‹ã€‚æ¯ç§é¢œè‰²å¯¹åº”ä¸€ç§ç›®æ ‡ç±»åˆ«ã€‚**


----------

#### 3.5 Preliminary ILSVRC results
#### 3.5 åˆæ­¥çš„ILSVRCç»“æœ

We applied the same network architecture we used for COCO to the ILSVRC DET dataset [16]. We train a SSD300 model using the ILSVRC2014 DET train and val1 as used in [22]. We first train the model with 10âˆ’3 learning rate for 320k iterations, and then continue training for 80k iterations with 10âˆ’4 and 40k iterations with 10âˆ’5. We can achieve 43.4 mAP on the val2 set [22]. Again, it validates that SSD is a general framework for high quality real-time detection.

**æˆ‘ä»¬å°†åœ¨COCOä¸Šåº”ç”¨çš„ç›¸åŒç½‘ç»œæ¶æ„åº”ç”¨äºILSVRC DETæ•°æ®é›†[16]ã€‚æˆ‘ä»¬ä½¿ç”¨[22]ä¸­ä½¿ç”¨çš„ILSVRC2014 DETtrainå’Œval1æ¥è®­ç»ƒSSD300æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆç”¨$10^{âˆ’3}$çš„å­¦ä¹ ç‡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¿›è¡Œäº†320kæ¬¡çš„è¿­ä»£ï¼Œç„¶åä»¥$10^{âˆ’4ç»§ç»­è¿­ä»£80kæ¬¡ï¼Œä»¥$10^âˆ’5$è¿­ä»£40kæ¬¡ã€‚æˆ‘ä»¬å¯ä»¥åœ¨val2æ•°æ®é›†ä¸Š[22]å®ç°43.4 mAPã€‚å†ä¸€æ¬¡è¯æ˜äº†SSDæ˜¯ç”¨äºé«˜è´¨é‡å®æ—¶æ£€æµ‹çš„é€šç”¨æ¡†æ¶ã€‚**


----------
#### 3.6 Data Augmentation for Small Object Accuracy
#### 3.6 ä¸ºå°ç›®æ ‡å‡†ç¡®ç‡è¿›è¡Œæ•°æ®å¢å¼º

Without a follow-up feature resampling step as in Faster R-CNN, the classification task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig. 4). The data augmentation strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as PASCAL VOC. The random crops generated by the strategy can be thought of as a â€zoom inâ€ operation and can generate many larger training examples. To implement a â€zoom outâ€ operation that creates more small training examples, we first randomly place an image on a canvas of 16Ã— of the original image size filled with mean values before we do any random crop operation. Because we have more training images by introducing this new â€expansionâ€ data aug- mentation trick, we have to double the training iterations. We have seen a consistent increase of 2%-3% mAP across multiple datasets, as shown in Table 6. In specific, Fig- ure 6 shows that the new augmentation trick significantly improves the performance on small objects. This result underscores the importance of the data augmentation strategy for the final model accuracy.

**SSDæ²¡æœ‰å¦‚Faster R-CNNä¸­åç»­çš„ç‰¹å¾é‡é‡‡æ ·æ­¥éª¤ï¼Œå°ç›®æ ‡çš„åˆ†ç±»ä»»åŠ¡å¯¹SSDæ¥è¯´ç›¸å¯¹å›°éš¾ï¼Œæ­£å¦‚æˆ‘ä»¬çš„åˆ†æï¼ˆè§å›¾4ï¼‰æ‰€ç¤ºã€‚2.2æè¿°çš„æ•°æ®å¢å¼ºæœ‰åŠ©äºæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨PASCAL VOCç­‰å°æ•°æ®é›†ä¸Šã€‚ç­–ç•¥äº§ç”Ÿçš„éšæœºè£å‰ªå¯ä»¥è¢«è®¤ä¸ºæ˜¯â€œæ”¾å¤§â€æ“ä½œï¼Œå¹¶ä¸”å¯ä»¥äº§ç”Ÿè®¸å¤šæ›´å¤§çš„è®­ç»ƒæ ·æœ¬ã€‚ä¸ºäº†å®ç°åˆ›å»ºæ›´å¤šå°å‹è®­ç»ƒæ ·æœ¬çš„â€œç¼©å°â€æ“ä½œï¼Œæˆ‘ä»¬é¦–å…ˆå°†å›¾åƒéšæœºæ”¾ç½®åœ¨å¡«å……äº†å¹³å‡å€¼çš„åŸå§‹å›¾åƒå¤§å°ä¸º16xçš„ç”»å¸ƒä¸Šï¼Œç„¶åå†è¿›è¡Œä»»æ„çš„éšæœºè£å‰ªæ“ä½œã€‚å› ä¸ºé€šè¿‡å¼•å…¥è¿™ä¸ªæ–°çš„â€œæ‰©å±•â€æ•°æ®å¢å¼ºæŠ€å·§ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„è®­ç»ƒå›¾åƒï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å°†è®­ç»ƒè¿­ä»£æ¬¡æ•°åŠ å€ã€‚æˆ‘ä»¬å·²ç»åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçœ‹åˆ°äº†ä¸€è‡´çš„2%âˆ’3%çš„mAPå¢é•¿ï¼Œå¦‚è¡¨6æ‰€ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œå›¾6æ˜¾ç¤ºæ–°çš„å¢å¼ºæŠ€å·§æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å°ç›®æ ‡ä¸Šçš„æ€§èƒ½ã€‚è¿™ä¸ªç»“æœå¼ºè°ƒäº†æ•°æ®å¢å¼ºç­–ç•¥å¯¹æœ€ç»ˆæ¨¡å‹ç²¾åº¦çš„é‡è¦æ€§ã€‚**

![11](/images/posts/ssd/11.png)

Table 6: Results on multiple datasets when we add the image expansion data aug-
mentation trick. SSD$300^*$ and SSD$512^*$ are the models that are trained with the new data augmentation.

**è¡¨6ï¼šæˆ‘ä»¬ä½¿ç”¨å›¾åƒæ‰©å±•æ•°æ®å¢å¼ºæŠ€å·§åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»“æœã€‚SSD$300^âˆ—$å’Œ$SSD512^âˆ—$æ˜¯ç”¨æ–°çš„æ•°æ®å¢å¼ºè®­ç»ƒçš„æ¨¡å‹ã€‚**

![12](/images/posts/ssd/12.png)

Fig.6: Sensitivity and impact of object size with new data augmentation on VOC2007 test set using [21]. The top row shows the effects of BBox Area per cat- egory for the original SSD300 and SSD512 model, and the bottom row corresponds to the SSD300* and SSD512* model trained with the new data augmentation trick. It is obvious that the new data augmentation trick helps detecting small objects significantly.

**å›¾6ï¼šå…·æœ‰æ–°çš„æ•°æ®å¢å¼ºçš„ç›®æ ‡å°ºå¯¸åœ¨[21]ä¸­ä½¿ç”¨çš„VOC2007testæ•°æ®é›†ä¸Šçµæ•åº¦åŠå½±å“ã€‚æœ€ä¸Šä¸€è¡Œæ˜¾ç¤ºäº†åŸå§‹SSD300å’ŒSSD512æ¨¡å‹ä¸Šæ¯ä¸ªç±»åˆ«çš„BBoxé¢ç§¯çš„å½±å“ï¼Œæœ€ä¸‹é¢ä¸€è¡Œå¯¹åº”ä½¿ç”¨æ–°çš„æ•°æ®å¢å¼ºè®­ç»ƒæŠ€å·§çš„SSD$300^âˆ—$å’Œ$SSD512^âˆ—$æ¨¡å‹ã€‚æ–°çš„æ•°æ®å¢å¼ºæŠ€å·§æ˜¾ç„¶æœ‰åŠ©äºæ˜¾è‘—æ£€æµ‹å°ç›®æ ‡ã€‚**

An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work.

**æ”¹è¿›SSDçš„å¦ä¸€ç§æ–¹æ³•æ˜¯è®¾è®¡ä¸€ä¸ªæ›´å¥½çš„é»˜è®¤è¾¹ç•Œæ¡†å¹³é“ºï¼Œä½¿å…¶ä½ç½®å’Œå°ºåº¦ä¸ç‰¹å¾æ˜ å°„ä¸Šæ¯ä¸ªä½ç½®çš„æ„Ÿå—é‡æ›´å¥½åœ°å¯¹é½ã€‚æˆ‘ä»¬å°†è¿™ä¸ªç•™ç»™æœªæ¥å·¥ä½œã€‚**


----------
#### 3.7 Inference time
#### 3.7 æ¨æ–­æ—¶é—´

Considering the large number of boxes generated from our method, it is essential to perform non-maximum suppression (nms) efficiently during inference. By using a con- fidence threshold of 0.01, we can filter out most boxes. We then apply nms with jaccard overlap of 0.45 per class and keep the top 200 detections per image. This step costs about 1.7 msec per image for SSD300 and 20 VOC classes, which is close to the total time (2.4 msec) spent on all newly added layers. We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz.

**è€ƒè™‘åˆ°æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿå¤§é‡è¾¹ç•Œæ¡†ï¼Œåœ¨æ¨æ–­æœŸé—´æ‰§è¡Œéæœ€å¤§å€¼æŠ‘åˆ¶ï¼ˆnmsï¼‰æ˜¯å¿…è¦çš„ã€‚é€šè¿‡ä½¿ç”¨0.01çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œæˆ‘ä»¬å¯ä»¥è¿‡æ»¤å¤§éƒ¨åˆ†è¾¹ç•Œæ¡†ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨nmsï¼Œæ¯ä¸ªç±»åˆ«0.45çš„Jaccardé‡å ï¼Œå¹¶ä¿ç•™æ¯å¼ å›¾åƒçš„å‰200ä¸ªæ£€æµ‹ã€‚å¯¹äºSSD300å’Œ20ä¸ªVOCç±»åˆ«ï¼Œè¿™ä¸ªæ­¥éª¤æ¯å¼ å›¾åƒèŠ±è´¹å¤§çº¦1.7æ¯«ç§’ï¼Œæ¥è¿‘åœ¨æ‰€æœ‰æ–°å¢å±‚ä¸ŠèŠ±è´¹çš„æ€»æ—¶é—´ï¼ˆ2.4æ¯«ç§’ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨Titan Xã€cuDNN v4ã€Intel Xeon E5-2667v3@3.20GHzä»¥åŠæ‰¹å¤§å°ä¸º8æ¥æµ‹é‡é€Ÿåº¦ã€‚**

Table 7 shows the comparison between SSD, Faster R-CNN[2], and YOLO[5]. Both our SSD300 and SSD512 method outperforms Faster R-CNN in both speed and accu- racy. Although Fast YOLO[5] can run at 155 FPS, it has lower accuracy by almost 22% mAP. To the best of our knowledge, SSD300 is the first real-time method to achieve above 70% mAP. Note that about 80% of the forward time is spent on the base network (VGG16 in our case). Therefore, using a faster base network could even further improve the speed, which can possibly make the SSD512 model real-time as well.

**è¡¨7æ˜¾ç¤ºäº†SSDï¼ŒFaster R-CNN[2]å’ŒYOLO[5]ä¹‹é—´çš„æ¯”è¾ƒã€‚æˆ‘ä»¬çš„SSD300å’ŒSSD512çš„é€Ÿåº¦å’Œç²¾åº¦å‡ä¼˜äºFaster R-CNNã€‚è™½ç„¶Fast YOLO[5]å¯ä»¥ä»¥155FPSçš„é€Ÿåº¦è¿è¡Œï¼Œä½†å…¶å‡†ç¡®æ€§å´é™ä½äº†è¿‘22%çš„mAPã€‚å°±æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSSD300æ˜¯ç¬¬ä¸€ä¸ªå®ç°70%ä»¥ä¸ŠmAPçš„å®æ—¶æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œå¤§çº¦80%å‰é¦ˆæ—¶é—´èŠ±è´¹åœ¨åŸºç¡€ç½‘ç»œä¸Šï¼ˆæœ¬ä¾‹ä¸­ä¸ºVGG16ï¼‰ã€‚å› æ­¤ï¼Œä½¿ç”¨æ›´å¿«çš„åŸºç¡€ç½‘ç»œå¯ä»¥è¿›ä¸€æ­¥æé«˜é€Ÿåº¦ï¼Œè¿™ä¹Ÿå¯èƒ½ä½¿SSD512æ¨¡å‹è¾¾åˆ°å®æ—¶ã€‚**

ï¼[13](/images/posts/ssd/13/png)

Table 7: Results on Pascal VOC2007 test. SSD300 is the only real-time detection method that can achieve above 70% mAP. By using a larger input image, SSD512 out- performs all methods on accuracy while maintaining a close to real-time speed.

**è¡¨7ï¼šPascal VOC2007 testä¸Šçš„ç»“æœã€‚SSD300æ˜¯å”¯ä¸€å¯ä»¥å–å¾—70%ä»¥ä¸ŠmAPçš„å®æ—¶æ£€æµ‹æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨æ›´å¤§çš„è¾“å…¥å›¾åƒï¼ŒSSD512åœ¨ç²¾åº¦ä¸Šè¶…è¿‡äº†æ‰€æœ‰æ–¹æ³•åŒæ—¶ä¿æŒè¿‘ä¼¼å®æ—¶çš„é€Ÿåº¦ã€‚**

### 4 Related Work
#### 4. ç›¸å…³å·¥ä½œ

There are two established classes of methods for object detection in images, one based on sliding windows and the other based on region proposal classification. Before the advent of convolutional neural networks, the state of the art for those two approaches â€“ Deformable Part Model (DPM) [26] and Selective Search [1] â€“ had comparable performance. However, after the dramatic improvement brought on by R-CNN [22], which combines selective search region proposals and convolutional network based post-classification, region proposal object detection methods became prevalent.

**åœ¨å›¾åƒä¸­æœ‰ä¸¤ç§å»ºç«‹çš„ç”¨äºç›®æ ‡æ£€æµ‹çš„æ–¹æ³•ï¼Œä¸€ç§åŸºäºæ»‘åŠ¨çª—å£ï¼Œå¦ä¸€ç§åŸºäºåŒºåŸŸæå‡ºåˆ†ç±»ã€‚åœ¨å·ç§¯ç¥ç»ç½‘ç»œå‡ºç°ä¹‹å‰ï¼Œè¿™ä¸¤ç§æ–¹æ³•çš„æœ€æ–°æŠ€æœ¯â€”â€”å¯å˜å½¢éƒ¨ä»¶æ¨¡å‹ï¼ˆDPMï¼‰[26]å’Œé€‰æ‹©æ€§æœç´¢[1]â€”â€”å…·æœ‰ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨R-CNN[22]ç»“åˆé€‰æ‹©æ€§æœç´¢åŒºåŸŸæå‡ºå’ŒåŸºäºååˆ†ç±»çš„å·ç§¯ç½‘ç»œå¸¦æ¥çš„æ˜¾è‘—æ”¹è¿›åï¼ŒåŒºåŸŸæå‡ºç›®æ ‡æ£€æµ‹æ–¹æ³•å˜å¾—æµè¡Œã€‚**


The original R-CNN approach has been improved in a variety of ways. The first set of approaches improve the quality and speed of post-classification, since it requires the classification of thousands of image crops, which is expensive and time-consuming. SPPnet [9] speeds up the original R-CNN approach significantly. It introduces a spatial pyramid pooling layer that is more robust to region size and scale and allows the classi- fication layers to reuse features computed over feature maps generated at several image resolutions. Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to- end by minimizing a loss for both confidences and bounding box regression, which was first introduced in MultiBox [7] for learning objectness.

**æœ€åˆçš„R-CNNæ–¹æ³•å·²ç»ä»¥å„ç§æ–¹å¼è¿›è¡Œäº†æ”¹è¿›ã€‚ç¬¬ä¸€å¥—æ–¹æ³•æé«˜äº†ååˆ†ç±»çš„è´¨é‡å’Œé€Ÿåº¦ï¼Œå› ä¸ºå®ƒéœ€è¦å¯¹æˆåƒä¸Šä¸‡çš„è£å‰ªå›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œè¿™æ˜¯æ˜‚è´µå’Œè€—æ—¶çš„ã€‚SPPnet[9]æ˜¾è‘—åŠ å¿«äº†åŸæœ‰çš„R-CNNæ–¹æ³•ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªç©ºé—´é‡‘å­—å¡”æ± åŒ–å±‚ï¼Œè¯¥å±‚å¯¹åŒºåŸŸå¤§å°å’Œå°ºåº¦æ›´é²æ£’ï¼Œå¹¶å…è®¸åˆ†ç±»å±‚é‡ç”¨å¤šä¸ªå›¾åƒåˆ†è¾¨ç‡ä¸‹ç”Ÿæˆçš„ç‰¹å¾æ˜ å°„ä¸Šè®¡ç®—çš„ç‰¹å¾ã€‚Fast R-CNN[6]æ‰©å±•äº†SPPnetï¼Œä½¿å¾—å®ƒå¯ä»¥é€šè¿‡æœ€å°åŒ–ç½®ä¿¡åº¦å’Œè¾¹ç•Œæ¡†å›å½’çš„æŸå¤±æ¥å¯¹æ‰€æœ‰å±‚è¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œæœ€åˆåœ¨MultiBox[7]ä¸­å¼•å…¥ç”¨äºå­¦ä¹ ç›®æ ‡ã€‚**

The second set of approaches improve the quality of proposal generation using deep neural networks. In the most recent works like MultiBox [7,8], the Selective Search region proposals, which are based on low-level image features, are replaced by pro- posals generated directly from a separate deep neural network. This further improves the detection accuracy but results in a somewhat complex setup, requiring the training of two neural networks with a dependency between them. Faster R-CNN [2] replaces selective search proposals by ones learned from a region proposal network (RPN), and introduces a method to integrate the RPN with Fast R-CNN by alternating between fine- tuning shared convolutional layers and prediction layers for these two networks. This way region proposals are used to pool mid-level features and the final classification step is less expensive. Our SSD is very similar to the region proposal network (RPN) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the RPN. But instead of using these to pool features and evaluate another classifier, we simultaneously produce a score for each object category in each box. Thus, our approach avoids the complication of merging RPN with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.

**ç¬¬äºŒå¥—æ–¹æ³•ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæé«˜äº†æå‡ºç”Ÿæˆçš„è´¨é‡ã€‚åœ¨æœ€è¿‘çš„å·¥ä½œMultiBox[7,8]ä¸­ï¼ŒåŸºäºä½çº§å›¾åƒç‰¹å¾çš„é€‰æ‹©æ€§æœç´¢åŒºåŸŸæå‡ºç›´æ¥è¢«å•ç‹¬çš„æ·±åº¦ç¥ç»ç½‘ç»œç”Ÿæˆçš„æå‡ºæ‰€å–ä»£ã€‚è¿™è¿›ä¸€æ­¥æé«˜äº†æ£€æµ‹ç²¾åº¦ï¼Œä½†æ˜¯å¯¼è‡´äº†ä¸€äº›å¤æ‚çš„è®¾ç½®ï¼Œéœ€è¦è®­ç»ƒä¸¤ä¸ªå…·æœ‰ä¾èµ–å…³ç³»çš„ç¥ç»ç½‘ç»œã€‚Faster R-CNN[2]å°†é€‰æ‹©æ€§æœç´¢æå‡ºæ›¿æ¢ä¸ºåŒºåŸŸæå‡ºç½‘ç»œï¼ˆRPNï¼‰å­¦ä¹ åˆ°çš„åŒºåŸŸæå‡ºï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿ä¸¤ä¸ªç½‘ç»œä¹‹é—´çš„å¾®è°ƒå…±äº«å·ç§¯å±‚å’Œé¢„æµ‹å±‚å°†RPNå’ŒFast R-CNNç»“åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½¿ç”¨åŒºåŸŸæå‡ºæ± åŒ–ä¸­çº§ç‰¹å¾ï¼Œå¹¶ä¸”æœ€åçš„åˆ†ç±»æ­¥éª¤æ¯”è¾ƒä¾¿å®œã€‚æˆ‘ä»¬çš„SSDä¸Faster R-CNNä¸­çš„åŒºåŸŸæå‡ºç½‘ç»œï¼ˆRPNï¼‰éå¸¸ç›¸ä¼¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿä½¿ç”¨ä¸€ç»„å›ºå®šçš„ï¼ˆé»˜è®¤ï¼‰è¾¹ç•Œæ¡†è¿›è¡Œé¢„æµ‹ï¼Œç±»ä¼¼äºRPNä¸­çš„é”šè¾¹ç•Œæ¡†ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨è¿™äº›æ¥æ± åŒ–ç‰¹å¾å¹¶è¯„ä¼°å¦ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªç›®æ ‡ç±»åˆ«åœ¨æ¯ä¸ªè¾¹ç•Œæ¡†ä¸­åŒæ—¶ç”Ÿæˆä¸€ä¸ªåˆ†æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¿å…äº†å°†RPNä¸Fast R-CNNåˆå¹¶çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”æ›´å®¹æ˜“è®­ç»ƒï¼Œæ›´å¿«ä¸”æ›´ç›´æ¥åœ°é›†æˆåˆ°å…¶å®ƒä»»åŠ¡ä¸­ã€‚**

Another set of methods, which are directly related to our approach, skip the proposal step altogether and predict bounding boxes and confidences for multiple categories di- rectly. OverFeat [4], a deep version of the sliding window method, predicts a bounding box directly from each location of the topmost feature map after knowing the confi- dences of the underlying object categories. YOLO [5] uses the whole topmost feature map to predict both confidences for multiple categories and bounding boxes (which are shared for these categories). Our SSD method falls in this category because we do not have the proposal step but use the default boxes. However, our approach is more flexible than the existing methods because we can use default boxes of different aspect ratios on each feature location from multiple feature maps at different scales. If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].

**ä¸æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥ç›¸å…³çš„å¦ä¸€ç»„æ–¹æ³•ï¼Œå®Œå…¨è·³è¿‡æå‡ºï¼ˆproposalï¼‰æ­¥éª¤ï¼Œç›´æ¥é¢„æµ‹å¤šä¸ªç±»åˆ«çš„è¾¹ç•Œæ¡†å’Œç½®ä¿¡åº¦ã€‚OverFeat[4]æ˜¯æ»‘åŠ¨çª—å£æ–¹æ³•çš„æ·±åº¦ç‰ˆæœ¬ï¼Œåœ¨çŸ¥é“äº†åº•å±‚ç›®æ ‡ç±»åˆ«çš„ç½®ä¿¡åº¦ä¹‹åï¼Œç›´æ¥ä»æœ€é¡¶å±‚çš„ç‰¹å¾æ˜ å°„çš„æ¯ä¸ªä½ç½®é¢„æµ‹è¾¹ç•Œæ¡†ã€‚YOLO[5]ä½¿ç”¨æ•´ä¸ªæœ€é¡¶å±‚çš„ç‰¹å¾æ˜ å°„æ¥é¢„æµ‹å¤šä¸ªç±»åˆ«å’Œè¾¹ç•Œæ¡†ï¼ˆè¿™äº›ç±»åˆ«å…±äº«ï¼‰çš„ç½®ä¿¡åº¦ã€‚æˆ‘ä»¬çš„SSDæ–¹æ³•å±äºè¿™ä¸€ç±»ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æå‡ºæ­¥éª¤ï¼Œä½†ä½¿ç”¨é»˜è®¤è¾¹ç•Œæ¡†ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•æ›´çµæ´»ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ä¸åŒå°ºåº¦çš„å¤šä¸ªç‰¹å¾æ˜ å°„çš„æ¯ä¸ªç‰¹å¾ä½ç½®ä¸Šä½¿ç”¨ä¸åŒé•¿å®½æ¯”çš„é»˜è®¤è¾¹ç•Œæ¡†ã€‚å¦‚æœæˆ‘ä»¬åªä»æœ€é¡¶å±‚çš„ç‰¹å¾æ˜ å°„çš„æ¯ä¸ªä½ç½®ä½¿ç”¨ä¸€ä¸ªé»˜è®¤æ¡†ï¼Œæˆ‘ä»¬çš„SSDå°†å…·æœ‰ä¸OverFeat[4]ç›¸ä¼¼çš„æ¶æ„ï¼›å¦‚æœæˆ‘ä»¬ä½¿ç”¨æ•´ä¸ªæœ€é¡¶å±‚çš„ç‰¹å¾æ˜ å°„ï¼Œå¹¶æ·»åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚è¿›è¡Œé¢„æµ‹æ¥ä»£æ›¿æˆ‘ä»¬çš„å·ç§¯é¢„æµ‹å™¨ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜ç¡®åœ°è€ƒè™‘å¤šä¸ªé•¿å®½æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥è¿‘ä¼¼åœ°å†ç°YOLO[5]ã€‚**


 ### 5 Conclusions
 This paper introduces SSD, a fast single-shot object detector for multiple categories. A key feature of our model is the use of multi-scale convolutional bounding box outputs attached to multiple feature maps at the top of the network. This representation allows us to efficiently model the space of possible box shapes. We experimentally validate that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance. We build SSD models with at least an order of magnitude more box predictions sampling location, scale, and aspect ratio, than existing methods [5,7]. We demonstrate that given the same VGG-16 base architecture, SSD compares favorably to its state-of-the-art object detector counterparts in terms of both accuracy and speed. Our SSD512 model significantly outperforms the state-of-the- art Faster R-CNN [2] in terms of accuracy on PASCAL VOC and COCO, while being 3Ã— faster. Our real time SSD300 model runs at 59 FPS, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.
 
**æœ¬æ–‡ä»‹ç»äº†SSDï¼Œä¸€ç§å¿«é€Ÿçš„å•æ¬¡å¤šç±»åˆ«ç›®æ ‡æ£€æµ‹å™¨ã€‚æˆ‘ä»¬æ¨¡å‹çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯ä½¿ç”¨ç½‘ç»œé¡¶éƒ¨å¤šä¸ªç‰¹å¾æ˜ å°„çš„å¤šå°ºåº¦å·ç§¯è¾¹ç•Œæ¡†è¾“å‡ºã€‚è¿™ç§è¡¨ç¤ºä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°å»ºæ¨¡å¯èƒ½çš„è¾¹ç•Œæ¡†å½¢çŠ¶ç©ºé—´ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯ï¼Œåœ¨ç»™å®šåˆé€‚è®­ç»ƒç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¤§é‡ä»”ç»†é€‰æ‹©çš„é»˜è®¤è¾¹ç•Œæ¡†ä¼šæé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æ„å»ºçš„SSDæ¨¡å‹æ¯”ç°æœ‰çš„æ–¹æ³•è‡³å°‘è¦å¤šä¸€ä¸ªæ•°é‡çº§çš„è¾¹ç•Œæ¡†é¢„æµ‹é‡‡æ ·ä½ç½®ï¼Œå°ºåº¦å’Œé•¿å®½æ¯”[5,7]ã€‚æˆ‘ä»¬è¯æ˜äº†ç»™å®šç›¸åŒçš„VGG-16åŸºç¡€æ¶æ„ï¼ŒSSDåœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦æ–¹é¢ä¸å…¶å¯¹åº”çš„æœ€å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹å™¨ç›¸æ¯”æ¯«ä¸é€Šè‰²ã€‚åœ¨PASCAL VOCå’ŒCOCOä¸Šï¼Œæˆ‘ä»¬çš„SSD512æ¨¡å‹çš„æ€§èƒ½æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„Faster R-CNN[2]ï¼Œè€Œé€Ÿåº¦æé«˜äº†3å€ã€‚æˆ‘ä»¬çš„å®æ—¶SSD300æ¨¡å‹è¿è¡Œé€Ÿåº¦ä¸º59FPSï¼Œæ¯”ç›®å‰çš„å®æ—¶YOLO[5]æ›´å¿«ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚**

Apart from its standalone utility, we believe that our monolithic and relatively sim- ple SSD model provides a useful building block for larger systems that employ an object detection component. A promising future direction is to explore its use as part of a sys- tem using recurrent neural networks to detect and track objects in video simultaneously.

**é™¤äº†å•ç‹¬ä½¿ç”¨ä¹‹å¤–ï¼Œæˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•´ä½“å’Œç›¸å¯¹ç®€å•çš„SSDæ¨¡å‹ä¸ºé‡‡ç”¨ç›®æ ‡æ£€æµ‹ç»„ä»¶çš„å¤§å‹ç³»ç»Ÿæä¾›äº†æœ‰ç”¨çš„æ„å»ºæ¨¡å—ã€‚ä¸€ä¸ªæœ‰å‰æ™¯çš„æœªæ¥æ–¹å‘æ˜¯æ¢ç´¢å®ƒä½œä¸ºç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œæ¥åŒæ—¶æ£€æµ‹å’Œè·Ÿè¸ªè§†é¢‘ä¸­çš„ç›®æ ‡ã€‚**

### 6 Acknowledgment
This work was started as an internship project at Google and continued at UNC. We would like to thank Alex Toshev for helpful discussions and are indebted to the Im- age Understanding and DistBelief teams at Google. We also thank Philip Ammirato and Patrick Poirson for helpful comments. We thank NVIDIA for providing GPUs and acknowledge support from NSF 1452851, 1446631, 1526367, 1533771.

#### 6. è‡´è°¢
è¿™é¡¹å·¥ä½œæ˜¯åœ¨è°·æ­Œçš„ä¸€ä¸ªå®ä¹ é¡¹ç›®å¼€å§‹çš„ï¼Œå¹¶åœ¨UNCç»§ç»­ã€‚æˆ‘ä»¬è¦æ„Ÿè°¢Alex Toshevè¿›è¡Œæœ‰ç›Šçš„è®¨è®ºï¼Œå¹¶æ„Ÿè°¢Googleçš„Image Understandingå’ŒDistBeliefå›¢é˜Ÿã€‚æˆ‘ä»¬ä¹Ÿæ„Ÿè°¢Philip Ammiratoå’ŒPatrick Poirsonæä¾›æœ‰ç”¨çš„æ„è§ã€‚æˆ‘ä»¬æ„Ÿè°¢NVIDIAæä¾›çš„GPUï¼Œå¹¶å¯¹NSF 1452851,1446631,1526367,1533771çš„æ”¯æŒè¡¨ç¤ºæ„Ÿè°¢ã€‚

References
1. Uijlings, J.R., van de Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object recognition. IJCV (2013)
2. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NIPS. (2015)
3. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:CVPR. (2016)
4. Sermanet,P.,Eigen,D.,Zhang,X.,Mathieu,M.,Fergus,R.,LeCun,Y.:Overfeat:Integrated recognition, localization and detection using convolutional networks. In: ICLR. (2014)
5. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: CVPR. (2016)
6. Girshick, R.: Fast R-CNN. In: ICCV. (2015)
7. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using deep
neural networks. In: CVPR. (2014)
8. Szegedy, C., Reed, S., Erhan, D., Anguelov, D.: Scalable, high-quality object detection.
arXiv preprint arXiv:1412.1441 v3 (2015)
9. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks
for visual recognition. In: ECCV. (2014)
10. Long,J.,Shelhamer,E.,Darrell,T.:Fullyconvolutionalnetworksforsemanticsegmentation.
In: CVPR. (2015)
11. Hariharan, B., Arbela Ìez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation
and fine-grained localization. In: CVPR. (2015)
12. Liu,W.,Rabinovich,A.,Berg,A.C.:ParseNet:Lookingwidertoseebetter.In:ILCR.(2016)
13. Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,Torralba,A.:Objectdetectorsemergeindeep
scene cnns. In: ICLR. (2015)
14. Howard, A.G.: Some improvements on deep convolutional neural network based image
classification. arXiv preprint arXiv:1312.5402 (2013)
15. Simonyan,K.,Zisserman,A.:Verydeepconvolutionalnetworksforlarge-scaleimagerecog-
nition. In: NIPS. (2015)
16. Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,
Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition
challenge. IJCV (2015)
17. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image seg-
mentation with deep convolutional nets and fully connected crfs. In: ICLR. (2015)
18. Holschneider,M.,Kronland-Martinet,R.,Morlet,J.,Tchamitchian,P.:Areal-timealgorithm for signal analysis with the help of the wavelet transform. In: Wavelets. Springer (1990)
286â€“297
19. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: MM. (2014)
20. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural
networks. In: AISTATS. (2010)
21. Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In: ECCV
2012. (2012)
22. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object
detection and semantic segmentation. In: CVPR. (2014)
23. Zhang, L., Lin, L., Liang, X., He, K.: Is faster r-cnn doing well for pedestrian detection. In:
ECCV. (2016)
24. Bell,S.,Zitnick,C.L.,Bala,K.,Girshick,R.:Inside-outsidenet:Detectingobjectsincontext
with skip pooling and recurrent neural networks. In: CVPR. (2016)
25. COCO: Common Objects in Context. http://mscoco.org/dataset/
#detections-leaderboard (2016) [Online; accessed 25-July-2016].
26. Felzenszwalb, P., McAllester, D., Ramanan, D.: A discriminatively trained, multiscale, de-
formable part model. In: CVPR. (2008)

